{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d054ee00",
   "metadata": {},
   "source": [
    "# Chapter 7 CNN \n",
    "\n",
    "CNN은 이미지 인식과 음성 인식 등에서 사용합니다. 특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 다 CNN을 기초로 합니다. \n",
    "\n",
    "CNN\n",
    "- CNN도 지금까지 본 신경망과 같이 계층을 조합하여 만들 수 있습니다. \n",
    "- 합송곱 계층/풀링 계층과 기존에 있던 신경망의 층으로 구성되어 있습니다. \n",
    "- CNN은 완전연결 신경망입니다. \n",
    "- 대략적 구조 \n",
    "    - ConV-ReLU- Pooling\n",
    "    - 기존 구조 Affine-Relu\n",
    "- 마지막 출력 계층에서는 Affine-Softmax 조합을 그대로 사용합니다\n",
    "\n",
    "완전연결 계층은 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있습니다. 그렇지만 데이터의 형상이 무시됩니다. 이미지를 예를 들면 완전연결 계층에 이미지 데이터를 입력할 때는 3차워 데이터를 평평한 1차원 데이터로 평탄해줘야 합니다. \n",
    "\n",
    "하지만 합성곱 계층은 형상을 유지합니다. 이미지도 3차워 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차원 데이터로 전달합니다. 그래서 이미지처럼 형상을 가진 데이터를 제대로 이해하는 것입니다. \n",
    "\n",
    "CNN에서는 합송곱 계층의 입출력 데이터를 특징 맵이라고도 합니다. 합성곱 계층의 입력 데이터를 입력 특징 맵, 출력 데이터를 출력 특징 맵이라고 합니다. \n",
    "\n",
    "합성곱 연산 \n",
    "- 합성곱 게층에서의 합송곱 연산을 처리합니다. 합성곱 연산은 이미지 처리에서 말하는 필터 연산에 해당합니다. \n",
    "- 합성곱은 element wise product으로 4x4 입력 데이터와 3x3 필터가 있을 시 2X2로 출력합니다. \n",
    "- 편향도 CNN에 존재합니다. 각 원소에 + bias를 합니다.\n",
    "\n",
    "패딩\n",
    "- 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(0)으로 채우기도 합니다 \n",
    "- 만약 폭 1짜리 패딩을 적용하면 입력 데이터 boundary에 0인 원소들이 새로 생깁니다. \n",
    "\n",
    "스트라이드 \n",
    "- 스트라이드는 sliding 윈도우와 비슷합니다. \n",
    "- 필터를 적용하는 위치의 간격입니다. \n",
    "- 스트라이드가 2로 하면 하나의 합성곱 연산에 들어가는 입력 데이터를 2칸식 이동하여 계산합니다. \n",
    "\n",
    "입력 크기(H, W)/필터 크기(FH, FW)/출력 크기(OH, OW)/패딩(P)/스트라이드(S)의 관계 \n",
    "\n",
    "$$ \n",
    "OH = \\frac{H + 2P - FH}{S} + 1 \n",
    "$$\n",
    "\n",
    "$$\n",
    "OW = \\frac{W + 2P - FW}{S} + 1 \n",
    "$$\n",
    "\n",
    "출력 크기가 정수가 아니면 오류를 내는 등의 대응이 필요합니다. 또한 값이 딱 나눠떨어지지 않을 때는 가까운 정수로 반올림하는 등 특별히 에러는 내지 않고 진행하도록 구현한 경우도 있습니다. \n",
    "\n",
    "3차원 데이터의 합성곱 연산 \n",
    "- 2차원 일때와 비교하면 길이 혹은 채널 방향으로 특징 맵이 늘어났습니다. 채널쪽으로 특징 맵이 여러 개 있으면 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고 그 결과를 더해서 하나의 출력을 얻습니다. \n",
    "- 3차원의 합성곱 연산에서 주의할 점은 입력 데이터의 채널 수와 필너의 채널 수가 같아야 합니다. \n",
    "\n",
    "배치 처리 \n",
    "- 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장합니다. 구체적으로는 데이터 수, 채널 수, 높이, 너비 \n",
    "- 배치 처리 시 데이터는 4차원 형상을 가진 채 각 계층을 타고 흐릅니다. \n",
    "- 신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이뤄진다는 것입니다. \n",
    "\n",
    "풀링 계층 \n",
    "- 풀링은 세로/가로 방향의 공간을 줄이는 연산입니다. \n",
    "- 풀링의 종류 \n",
    "    - 최대 풀링\n",
    "        - 스트라이드를 n으로 처리하여 nxn중 가장 큰 값을 가져와 nxn 출력층을 만듭니다\n",
    "    - 평균 풀링 \n",
    "        - 최대 풀링과 다르게 평균값을 가지고 옵니다 \n",
    "- 풀링은 일반적으로 윈도우 크기와 스트라이드를 동일 하개 합니다. \n",
    "- 특징 \n",
    "    - 학습해야 할 매개변수가 없다 \n",
    "    - 채널 수가 변하지 않는다 \n",
    "    - 입력의 변화에 영향을 적게 받는다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d94c35b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n",
      "(1, 28, 28)\n",
      "(1, 28, 28)\n",
      "[[0.14757551 0.72285559 0.91968674 0.84133326 0.61783009 0.16256359\n",
      "  0.43606095 0.1312974  0.91275817 0.02714945 0.45486756 0.32580759\n",
      "  0.11908366 0.6224879  0.85888689 0.11567667 0.26242878 0.20650811\n",
      "  0.06887444 0.77252336 0.69972839 0.21961892 0.41686175 0.33786246\n",
      "  0.2736403  0.88637276 0.9305572  0.8627709 ]\n",
      " [0.86733769 0.53135643 0.14252406 0.56338745 0.22084884 0.37044643\n",
      "  0.01303456 0.69097641 0.43126456 0.29740436 0.86196955 0.95476431\n",
      "  0.86780301 0.51456233 0.67540142 0.2123251  0.21909841 0.79802107\n",
      "  0.05454144 0.98678971 0.19905217 0.40455133 0.01430747 0.16509131\n",
      "  0.45292082 0.28487699 0.03523476 0.7184923 ]\n",
      " [0.70859481 0.12918629 0.49905056 0.0924552  0.73924721 0.29057954\n",
      "  0.59425281 0.26788643 0.35132501 0.47356686 0.82109923 0.873001\n",
      "  0.6390603  0.58839852 0.60406699 0.01165664 0.7217198  0.54829709\n",
      "  0.81161805 0.07836216 0.66843341 0.77866737 0.87457341 0.67764054\n",
      "  0.93132953 0.52193793 0.22701787 0.82149681]\n",
      " [0.89029719 0.56553602 0.5441723  0.36676924 0.02439482 0.45263514\n",
      "  0.12009728 0.4155751  0.83249112 0.77638073 0.32741044 0.05842409\n",
      "  0.54884897 0.09938769 0.90043588 0.03208944 0.86329994 0.64394913\n",
      "  0.23658593 0.19309017 0.58744967 0.1288184  0.75867373 0.68830276\n",
      "  0.98210812 0.5269197  0.98910014 0.92509675]\n",
      " [0.55284653 0.52561124 0.73080661 0.52368508 0.76673939 0.42821172\n",
      "  0.61745793 0.75435572 0.70702046 0.68647229 0.95749015 0.37623424\n",
      "  0.37045611 0.34992121 0.37229995 0.03568691 0.21617571 0.66472935\n",
      "  0.08511156 0.6577371  0.79170883 0.54726036 0.96286619 0.72344276\n",
      "  0.16309526 0.55964349 0.17578572 0.57888337]\n",
      " [0.22754873 0.96905544 0.25043543 0.49369234 0.31474201 0.33212406\n",
      "  0.93812535 0.39105444 0.30570111 0.65023073 0.48702667 0.56664205\n",
      "  0.83741404 0.52766789 0.17777665 0.09064462 0.18102425 0.60254042\n",
      "  0.71610504 0.66076193 0.82318385 0.07308469 0.58251093 0.57112314\n",
      "  0.01783755 0.05570973 0.11078078 0.30318318]\n",
      " [0.54707401 0.32683045 0.99477752 0.56109312 0.85128628 0.52831149\n",
      "  0.02857161 0.14347551 0.12557491 0.01827072 0.85751838 0.88186788\n",
      "  0.85337436 0.83233933 0.80512413 0.65317313 0.35642742 0.01027918\n",
      "  0.66932131 0.78378969 0.23713908 0.75760036 0.87591378 0.15041244\n",
      "  0.27906407 0.52557292 0.59231643 0.50679216]\n",
      " [0.97703337 0.20593816 0.54257434 0.6052485  0.30600218 0.07986469\n",
      "  0.34472441 0.42880335 0.79979318 0.21679032 0.15610153 0.47643697\n",
      "  0.83902465 0.03865101 0.45043934 0.36587188 0.55992279 0.49594056\n",
      "  0.96389043 0.33235075 0.82735721 0.13893396 0.31530607 0.53058654\n",
      "  0.28248749 0.31976156 0.50907031 0.2701466 ]\n",
      " [0.42240194 0.10135761 0.08628538 0.99056736 0.07333452 0.96094239\n",
      "  0.35534787 0.26899522 0.17560579 0.86133259 0.54828985 0.21343463\n",
      "  0.50446109 0.77682447 0.13385362 0.15615402 0.88732447 0.23091764\n",
      "  0.20110266 0.07503357 0.5083929  0.90180333 0.88250226 0.59878681\n",
      "  0.26037756 0.83134313 0.17221565 0.30662676]\n",
      " [0.96072574 0.74930701 0.88433043 0.86257519 0.8509884  0.35474981\n",
      "  0.29058466 0.61199489 0.63725769 0.63541009 0.35791981 0.08763305\n",
      "  0.56132686 0.50691777 0.55341341 0.41193408 0.80948344 0.92083581\n",
      "  0.03348197 0.12403624 0.55470851 0.61962173 0.30297518 0.02176708\n",
      "  0.08408171 0.74489555 0.83682007 0.11977132]\n",
      " [0.85191966 0.68320649 0.67314071 0.44615919 0.5204486  0.55041512\n",
      "  0.63159452 0.83466844 0.17362425 0.23044217 0.39780921 0.53620541\n",
      "  0.31920001 0.62024925 0.91955009 0.57341804 0.12621863 0.77625757\n",
      "  0.31515741 0.6624091  0.07991741 0.97225103 0.00208039 0.47116492\n",
      "  0.80013721 0.90494642 0.16257302 0.8130357 ]\n",
      " [0.06761974 0.00976354 0.11944642 0.34755092 0.99084436 0.01632606\n",
      "  0.9854629  0.15220447 0.31899776 0.01055854 0.36932868 0.39738221\n",
      "  0.6274251  0.01198531 0.66379646 0.72385982 0.7305733  0.81575157\n",
      "  0.90392156 0.03283015 0.03886887 0.79360747 0.25429608 0.23210516\n",
      "  0.76104363 0.18882232 0.34225096 0.30221308]\n",
      " [0.37564544 0.95069508 0.62391779 0.90444227 0.39862721 0.49848149\n",
      "  0.17681569 0.8623378  0.97714493 0.47315172 0.02016819 0.37743605\n",
      "  0.97511273 0.42057378 0.1274446  0.99799215 0.39442555 0.52656408\n",
      "  0.55048578 0.92139994 0.56906128 0.98820487 0.53047042 0.5206228\n",
      "  0.82455585 0.51791073 0.55510482 0.93410743]\n",
      " [0.65224339 0.36557852 0.03143192 0.01587071 0.72935244 0.1481036\n",
      "  0.37063175 0.25394038 0.62335013 0.02994975 0.07646593 0.97832906\n",
      "  0.69894044 0.58462903 0.69896665 0.24705269 0.0386155  0.64935341\n",
      "  0.38784716 0.49350922 0.92556197 0.65527466 0.91636061 0.41528379\n",
      "  0.49410938 0.91701769 0.83909714 0.28288731]\n",
      " [0.98471728 0.82306164 0.64722939 0.11101448 0.39332069 0.70042168\n",
      "  0.77714698 0.39220806 0.80795889 0.55366342 0.0347849  0.19703427\n",
      "  0.44850485 0.8109887  0.57042539 0.25277921 0.13394573 0.09547819\n",
      "  0.00541336 0.10737912 0.57277546 0.34830651 0.65141768 0.09746867\n",
      "  0.45238894 0.20297345 0.90254403 0.60628451]\n",
      " [0.55506356 0.56972453 0.72541817 0.77247762 0.40687852 0.95883047\n",
      "  0.59867446 0.45522284 0.96696265 0.05496937 0.75465112 0.70135402\n",
      "  0.54552453 0.14116305 0.31860322 0.71418245 0.56793492 0.90501214\n",
      "  0.49019549 0.99362136 0.91530862 0.42816682 0.58103167 0.66306563\n",
      "  0.21323596 0.89457006 0.94441902 0.42070041]\n",
      " [0.83629952 0.83665441 0.97146762 0.88981633 0.77729278 0.21685488\n",
      "  0.25343542 0.15213914 0.3497169  0.86232323 0.30819833 0.3288678\n",
      "  0.48541246 0.63284625 0.23488962 0.85115679 0.49075173 0.80905161\n",
      "  0.57205012 0.81731707 0.46500333 0.55738834 0.07434788 0.53099853\n",
      "  0.61860612 0.87499086 0.79753321 0.26792863]\n",
      " [0.92099823 0.47076942 0.36513978 0.24877059 0.30756232 0.3282801\n",
      "  0.74896511 0.6013095  0.67595578 0.67817677 0.5105017  0.48822425\n",
      "  0.23450694 0.03416054 0.13139416 0.54602864 0.12529357 0.90367261\n",
      "  0.34920997 0.15681916 0.9364248  0.91193505 0.39973838 0.87736476\n",
      "  0.3657059  0.54370386 0.09232223 0.41778323]\n",
      " [0.44391414 0.48118526 0.57595393 0.58237028 0.16350024 0.51504207\n",
      "  0.58162003 0.85572984 0.67119145 0.75995543 0.9011259  0.59644662\n",
      "  0.35703221 0.25935173 0.75056942 0.84188238 0.5408556  0.5685887\n",
      "  0.22156826 0.07397137 0.5411991  0.74286248 0.04670749 0.42743341\n",
      "  0.15294824 0.68587192 0.07002084 0.82703295]\n",
      " [0.37620203 0.1053287  0.49634902 0.53985818 0.40117754 0.51677921\n",
      "  0.16079223 0.19025773 0.38259074 0.42310916 0.38148213 0.7932186\n",
      "  0.84875325 0.06001837 0.37475666 0.0932363  0.48856561 0.13329789\n",
      "  0.6971279  0.83281322 0.67754725 0.88357553 0.98801257 0.12802102\n",
      "  0.52922772 0.18445629 0.95557884 0.15631119]\n",
      " [0.06670153 0.3248999  0.245621   0.36399667 0.68881433 0.65559821\n",
      "  0.44448048 0.81365293 0.61101482 0.88604938 0.07987188 0.39165586\n",
      "  0.22285062 0.1474663  0.30706007 0.0233236  0.3522002  0.63912871\n",
      "  0.9746533  0.67141067 0.36093018 0.85214818 0.8976108  0.79789803\n",
      "  0.22361775 0.51722594 0.40607803 0.36798068]\n",
      " [0.81941801 0.3454629  0.1318046  0.14289227 0.75814252 0.5325585\n",
      "  0.36243582 0.2852853  0.00227824 0.65915547 0.91308269 0.24626615\n",
      "  0.53602232 0.45233452 0.90392688 0.29278243 0.9949891  0.61833806\n",
      "  0.44424845 0.37711979 0.84934296 0.31077995 0.50134645 0.11557247\n",
      "  0.9692439  0.830205   0.97662284 0.98294087]\n",
      " [0.27409074 0.24861722 0.04818652 0.40050213 0.29516331 0.65441985\n",
      "  0.48596658 0.55956641 0.27826233 0.38097853 0.78842534 0.61373752\n",
      "  0.44916109 0.36503385 0.69059902 0.86172788 0.46357911 0.38034932\n",
      "  0.9447227  0.53789013 0.45919846 0.5625493  0.61522648 0.39571749\n",
      "  0.63195441 0.78886723 0.46048736 0.45114234]\n",
      " [0.19491545 0.24237579 0.75355032 0.15052199 0.9265243  0.7233459\n",
      "  0.06614757 0.29515457 0.2296896  0.76525404 0.13196157 0.56629179\n",
      "  0.45220605 0.37585025 0.12279332 0.83861808 0.27736155 0.19824921\n",
      "  0.97301692 0.91238778 0.09283915 0.90526737 0.82958741 0.73500999\n",
      "  0.74683252 0.12308221 0.68132973 0.27947171]\n",
      " [0.59127619 0.59277422 0.53732905 0.28651263 0.7298701  0.44147486\n",
      "  0.81342819 0.83909963 0.36377501 0.6555735  0.90420858 0.14167846\n",
      "  0.96374101 0.76598758 0.23839096 0.45060673 0.33971018 0.86680562\n",
      "  0.57657807 0.75993383 0.42671183 0.03326463 0.5834893  0.61188406\n",
      "  0.14323001 0.29325001 0.3355868  0.27870194]\n",
      " [0.6077959  0.78180289 0.86015522 0.5298418  0.62466992 0.18078857\n",
      "  0.91456896 0.5878205  0.10719696 0.03204263 0.26950693 0.78244132\n",
      "  0.80487462 0.26508913 0.9510067  0.19077232 0.74202728 0.87499075\n",
      "  0.27427696 0.60521833 0.98835004 0.78252828 0.06926356 0.34514846\n",
      "  0.38318374 0.29193041 0.54616515 0.71796446]\n",
      " [0.77715285 0.72443844 0.94286392 0.55538444 0.27678295 0.86451042\n",
      "  0.23663488 0.57381912 0.60709555 0.43564975 0.52791618 0.23632555\n",
      "  0.23115324 0.72805051 0.48559598 0.68765006 0.91620896 0.76350425\n",
      "  0.74611581 0.50747857 0.42755883 0.57425353 0.704743   0.61464418\n",
      "  0.05709431 0.84407874 0.35198637 0.5898806 ]\n",
      " [0.94450461 0.77187861 0.83811746 0.88912733 0.25219179 0.70343545\n",
      "  0.52741764 0.40443754 0.60010477 0.25318177 0.8519093  0.4815568\n",
      "  0.09556815 0.953423   0.05031778 0.27916756 0.44166108 0.34265799\n",
      "  0.55046755 0.82386214 0.6493935  0.59644476 0.94226687 0.05178703\n",
      "  0.05295453 0.74633919 0.65563163 0.14868304]]\n"
     ]
    }
   ],
   "source": [
    "##합성곱 풀링 계층 구현하기 \n",
    "## 4차원 데이터 배열 \n",
    "import numpy as np \n",
    "x = np.random.rand(10, 1, 28, 28)\n",
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "# 첫 채널 공간 데이터 \n",
    "print(x[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063bd128",
   "metadata": {},
   "source": [
    "im2col로 데이터 전개하기 \n",
    "- 합성곱 연산을 FOR 문으로 구연시 성능이 떨어집니다. \n",
    "- 그러므로 3/4 차원데이터를 2차원으로 표현합니다. \n",
    "- IMAGE TO COLUMN\n",
    "- im2col로 입력 데이터를 전개한 다음에는 합성곱 계층의 필터를 1열로 전개하고 두 행렬의 곱을 계산하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471b147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "## 합성곱 계층 구현하기 \n",
    "\n",
    "import sys, os \n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col \n",
    "\n",
    "x1 = np.random.rand(1,3,7,7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x1 = np.random.rand(10,3,7,7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5048d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "908a0a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 1.1384462146795284e-05\n",
      "b1 4.311173326981536e-05\n",
      "W2 6.134188714137366e-11\n",
      "b2 6.0310795696114085e-09\n",
      "W3 1.4802555928476334e-10\n",
      "b3 1.7989118968469996e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,10, 10), \n",
    "                        conv_param = {'filter_num':10, 'filter_size':3, 'pad':0, 'stride':1},\n",
    "                        hidden_size=10, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "X = np.random.rand(100).reshape((1, 1, 10, 10))\n",
    "T = np.array([1]).reshape((1,1))\n",
    "\n",
    "grad_num = network.numerical_gradient(X, T)\n",
    "grad = network.gradient(X, T)\n",
    "\n",
    "for key, val in grad_num.items():\n",
    "    print(key, np.abs(grad_num[key] - grad[key]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7651b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3002611438427842\n",
      "=== epoch:1, train acc:0.251, test acc:0.289 ===\n",
      "train loss:2.298037887194883\n",
      "train loss:2.2943628265203286\n",
      "train loss:2.2914230984562045\n",
      "train loss:2.2853682414587815\n",
      "train loss:2.276819710128319\n",
      "train loss:2.264220493704185\n",
      "train loss:2.248528674565092\n",
      "train loss:2.2392465602416425\n",
      "train loss:2.2124984189577233\n",
      "train loss:2.1861350970537416\n",
      "train loss:2.154821923772509\n",
      "train loss:2.1427054789505053\n",
      "train loss:2.070504646054822\n",
      "train loss:2.042814037444307\n",
      "train loss:2.029210364979061\n",
      "train loss:1.9259827538793362\n",
      "train loss:1.897181947984329\n",
      "train loss:1.7701572344859167\n",
      "train loss:1.7593862075140434\n",
      "train loss:1.6508260752832158\n",
      "train loss:1.5546891445201134\n",
      "train loss:1.4998116079172374\n",
      "train loss:1.3646102486990304\n",
      "train loss:1.3025424345129661\n",
      "train loss:1.2360402515919244\n",
      "train loss:1.2060971343755316\n",
      "train loss:1.1492776219919536\n",
      "train loss:0.9589537266404667\n",
      "train loss:1.0183751518861914\n",
      "train loss:1.045462908792719\n",
      "train loss:0.9473189079122486\n",
      "train loss:0.9489261184087029\n",
      "train loss:0.7118088737736781\n",
      "train loss:0.7898336074982457\n",
      "train loss:0.6703311268676734\n",
      "train loss:0.7694678894818355\n",
      "train loss:0.7234080869284939\n",
      "train loss:0.6646888755046031\n",
      "train loss:0.6130340671519732\n",
      "train loss:0.7679808460611071\n",
      "train loss:0.6283520669694976\n",
      "train loss:0.6652080598070235\n",
      "train loss:0.6598716157240391\n",
      "train loss:0.7248832702279533\n",
      "train loss:0.5519049398101377\n",
      "train loss:0.6952725214182003\n",
      "train loss:0.5898961340072787\n",
      "train loss:0.44073849623548095\n",
      "train loss:0.7943894512623217\n",
      "train loss:0.5679543829414324\n",
      "=== epoch:2, train acc:0.81, test acc:0.788 ===\n",
      "train loss:0.39703172469294606\n",
      "train loss:0.3752204723736772\n",
      "train loss:0.5247726647294693\n",
      "train loss:0.4371094438843649\n",
      "train loss:0.5282631203589027\n",
      "train loss:0.4123487779278936\n",
      "train loss:0.3895479764485044\n",
      "train loss:0.48237838235010605\n",
      "train loss:0.5775686074793328\n",
      "train loss:0.5817049064051145\n",
      "train loss:0.4678407676479246\n",
      "train loss:0.42451091228691046\n",
      "train loss:0.4982450689083431\n",
      "train loss:0.32370918514244595\n",
      "train loss:0.3962532473809175\n",
      "train loss:0.36713545399175823\n",
      "train loss:0.45596473683667954\n",
      "train loss:0.4021670232772687\n",
      "train loss:0.46366327657812795\n",
      "train loss:0.35033313612096834\n",
      "train loss:0.5314673194840782\n",
      "train loss:0.35503985667001225\n",
      "train loss:0.4143235240515308\n",
      "train loss:0.4361035233771694\n",
      "train loss:0.5942139500810679\n",
      "train loss:0.47620582392870764\n",
      "train loss:0.35812904534063605\n",
      "train loss:0.6002134521524932\n",
      "train loss:0.38389704356960963\n",
      "train loss:0.5153483910267128\n",
      "train loss:0.39186845118099944\n",
      "train loss:0.44557686743173297\n",
      "train loss:0.3779323938506542\n",
      "train loss:0.35584398729315064\n",
      "train loss:0.5264991542140277\n",
      "train loss:0.3707984132807598\n",
      "train loss:0.42869373620815937\n",
      "train loss:0.4699744787264876\n",
      "train loss:0.31257281916088664\n",
      "train loss:0.286300051910687\n",
      "train loss:0.27376144845467903\n",
      "train loss:0.44364567595115906\n",
      "train loss:0.38318042222024606\n",
      "train loss:0.2773495754615427\n",
      "train loss:0.4587078124985634\n",
      "train loss:0.3392121668869577\n",
      "train loss:0.3727175677092026\n",
      "train loss:0.33148063231524333\n",
      "train loss:0.31562542528353227\n",
      "train loss:0.2952257666474499\n",
      "=== epoch:3, train acc:0.889, test acc:0.88 ===\n",
      "train loss:0.2320680903888234\n",
      "train loss:0.3169843972043182\n",
      "train loss:0.4305361114657806\n",
      "train loss:0.25671793802338616\n",
      "train loss:0.3635710124596098\n",
      "train loss:0.2906096576829536\n",
      "train loss:0.33759926150966835\n",
      "train loss:0.4067875483259752\n",
      "train loss:0.31106942432164975\n",
      "train loss:0.2404372505235056\n",
      "train loss:0.2087801696123552\n",
      "train loss:0.2403636865354969\n",
      "train loss:0.26237874781093334\n",
      "train loss:0.32920534622292086\n",
      "train loss:0.12220089840060315\n",
      "train loss:0.35872041801257554\n",
      "train loss:0.3908272418380718\n",
      "train loss:0.3346140318314\n",
      "train loss:0.34647736793904294\n",
      "train loss:0.3153885828527839\n",
      "train loss:0.3540409686816403\n",
      "train loss:0.3079523971410759\n",
      "train loss:0.21657453536056745\n",
      "train loss:0.1685534635212065\n",
      "train loss:0.22774379267328315\n",
      "train loss:0.3363597197454814\n",
      "train loss:0.33898367831142456\n",
      "train loss:0.44210708306590574\n",
      "train loss:0.3579838409669814\n",
      "train loss:0.27075418136157864\n",
      "train loss:0.3132980100238339\n",
      "train loss:0.29813886744258555\n",
      "train loss:0.21761130167643133\n",
      "train loss:0.2433139195316653\n",
      "train loss:0.220328696929428\n",
      "train loss:0.3414771956191078\n",
      "train loss:0.14607779182178393\n",
      "train loss:0.23435255396724275\n",
      "train loss:0.35746723148105486\n",
      "train loss:0.375359035649903\n",
      "train loss:0.3660702192128641\n",
      "train loss:0.21611279377246348\n",
      "train loss:0.23673348525882987\n",
      "train loss:0.22616904291392387\n",
      "train loss:0.35867964283360015\n",
      "train loss:0.4357307099046101\n",
      "train loss:0.21319953193836866\n",
      "train loss:0.20272147700676965\n",
      "train loss:0.3610492669153533\n",
      "train loss:0.2585795829262899\n",
      "=== epoch:4, train acc:0.905, test acc:0.877 ===\n",
      "train loss:0.28839052130370196\n",
      "train loss:0.3114067724755022\n",
      "train loss:0.11718609358570335\n",
      "train loss:0.5132988500824489\n",
      "train loss:0.35665995106923654\n",
      "train loss:0.3144897751206186\n",
      "train loss:0.2522152595705142\n",
      "train loss:0.2777519444959889\n",
      "train loss:0.28816243202495856\n",
      "train loss:0.2616865944292665\n",
      "train loss:0.3958393667402153\n",
      "train loss:0.2025049649430991\n",
      "train loss:0.23378953392476298\n",
      "train loss:0.29637892100217533\n",
      "train loss:0.22688993819517012\n",
      "train loss:0.3099111816206894\n",
      "train loss:0.19536023482276524\n",
      "train loss:0.19947596615569171\n",
      "train loss:0.21413031184747344\n",
      "train loss:0.21233567116800522\n",
      "train loss:0.18968300346702732\n",
      "train loss:0.2325599978702658\n",
      "train loss:0.2026960631893284\n",
      "train loss:0.2759491606317225\n",
      "train loss:0.2669321767958471\n",
      "train loss:0.23776170061174617\n",
      "train loss:0.2755346146299528\n",
      "train loss:0.18890017260738876\n",
      "train loss:0.277591479004589\n",
      "train loss:0.25370175423569813\n",
      "train loss:0.24580564060079024\n",
      "train loss:0.24155538082013484\n",
      "train loss:0.24703207205357408\n",
      "train loss:0.13317162042691005\n",
      "train loss:0.27864644577238656\n",
      "train loss:0.23570304847733123\n",
      "train loss:0.2294926045365544\n",
      "train loss:0.21833572171125323\n",
      "train loss:0.2218025739995479\n",
      "train loss:0.34701966360011643\n",
      "train loss:0.21766227594788906\n",
      "train loss:0.3141915304690441\n",
      "train loss:0.2708959156298193\n",
      "train loss:0.2441418461494299\n",
      "train loss:0.1665856874479512\n",
      "train loss:0.275629655952031\n",
      "train loss:0.21842846933829457\n",
      "train loss:0.15190117982121362\n",
      "train loss:0.28764859639914486\n",
      "train loss:0.1751194137899284\n",
      "=== epoch:5, train acc:0.921, test acc:0.902 ===\n",
      "train loss:0.24184850420624127\n",
      "train loss:0.10322761463337395\n",
      "train loss:0.1209565868897053\n",
      "train loss:0.2626718837169474\n",
      "train loss:0.2047141545444059\n",
      "train loss:0.22848769494089108\n",
      "train loss:0.14215542266549872\n",
      "train loss:0.2176931969428915\n",
      "train loss:0.23552854781459195\n",
      "train loss:0.21464738146738419\n",
      "train loss:0.1692214955100596\n",
      "train loss:0.2606396543497266\n",
      "train loss:0.13649854982688894\n",
      "train loss:0.23285682420114567\n",
      "train loss:0.11446524226428441\n",
      "train loss:0.19586548887151062\n",
      "train loss:0.13054833706783095\n",
      "train loss:0.12768146697591806\n",
      "train loss:0.1986610144945459\n",
      "train loss:0.2419452944503237\n",
      "train loss:0.22096348929050513\n",
      "train loss:0.24451173471892865\n",
      "train loss:0.16487313935195444\n",
      "train loss:0.2136042220744513\n",
      "train loss:0.272468010921896\n",
      "train loss:0.16982066195848047\n",
      "train loss:0.20606681971673813\n",
      "train loss:0.2625569826709504\n",
      "train loss:0.21942656319955783\n",
      "train loss:0.20731932973977185\n",
      "train loss:0.20648540753376646\n",
      "train loss:0.18202837185985668\n",
      "train loss:0.21505122431617094\n",
      "train loss:0.2631630283353298\n",
      "train loss:0.18818532064409244\n",
      "train loss:0.24694508317476938\n",
      "train loss:0.12006172193656334\n",
      "train loss:0.2795129896856958\n",
      "train loss:0.11794310676700297\n",
      "train loss:0.1589090750256148\n",
      "train loss:0.1872240112550941\n",
      "train loss:0.2109991730494294\n",
      "train loss:0.17554167865528142\n",
      "train loss:0.10982143183573745\n",
      "train loss:0.23554714644741565\n",
      "train loss:0.07573714011586632\n",
      "train loss:0.18583988225273934\n",
      "train loss:0.16555803590052157\n",
      "train loss:0.20903253101261993\n",
      "train loss:0.20574659005853152\n",
      "=== epoch:6, train acc:0.934, test acc:0.923 ===\n",
      "train loss:0.11126184043104152\n",
      "train loss:0.14588381999851893\n",
      "train loss:0.15109847381372765\n",
      "train loss:0.216594890632643\n",
      "train loss:0.186637706831838\n",
      "train loss:0.11806916505958986\n",
      "train loss:0.12261002594752368\n",
      "train loss:0.19530894239328458\n",
      "train loss:0.17695784842889584\n",
      "train loss:0.23050268669530433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.13577178952755098\n",
      "train loss:0.10069112024693697\n",
      "train loss:0.22864086594382907\n",
      "train loss:0.15261827693395721\n",
      "train loss:0.22652830250614497\n",
      "train loss:0.15957363203247357\n",
      "train loss:0.09142661922852616\n",
      "train loss:0.14674901455984768\n",
      "train loss:0.11854628942144217\n",
      "train loss:0.24869775384945744\n",
      "train loss:0.1038459105274113\n",
      "train loss:0.17100856866931644\n",
      "train loss:0.15514755743171066\n",
      "train loss:0.22422247343872617\n",
      "train loss:0.18151828824927993\n",
      "train loss:0.14238874209433783\n",
      "train loss:0.11848602447958186\n",
      "train loss:0.17008693677187417\n",
      "train loss:0.18062711046655663\n",
      "train loss:0.2352042339315114\n",
      "train loss:0.10110259700247326\n",
      "train loss:0.12992135697952148\n",
      "train loss:0.2305715325787672\n",
      "train loss:0.0930755179808311\n",
      "train loss:0.23948075157490523\n",
      "train loss:0.1361133863003254\n",
      "train loss:0.1671418915562725\n",
      "train loss:0.11287627417886309\n",
      "train loss:0.10174497001634071\n",
      "train loss:0.20199909763075147\n",
      "train loss:0.08778525601486599\n",
      "train loss:0.1550893189495027\n",
      "train loss:0.11087799280404977\n",
      "train loss:0.15986915853836342\n",
      "train loss:0.15157736093459634\n",
      "train loss:0.12223165312061536\n",
      "train loss:0.2625880891830719\n",
      "train loss:0.22095453264749626\n",
      "train loss:0.09408771343984002\n",
      "train loss:0.07975247878810164\n",
      "=== epoch:7, train acc:0.944, test acc:0.919 ===\n",
      "train loss:0.14405123109618403\n",
      "train loss:0.10116038303352308\n",
      "train loss:0.13652359837213648\n",
      "train loss:0.19495240203638853\n",
      "train loss:0.09180503169519377\n",
      "train loss:0.14650508866157755\n",
      "train loss:0.1173036476875794\n",
      "train loss:0.17218890772665332\n",
      "train loss:0.1777596865624053\n",
      "train loss:0.1469303966558428\n",
      "train loss:0.13176116815553052\n",
      "train loss:0.12463156755564958\n",
      "train loss:0.1329164721821763\n",
      "train loss:0.15944960604428168\n",
      "train loss:0.07074065580658655\n",
      "train loss:0.11318069391698819\n",
      "train loss:0.10714657736569827\n",
      "train loss:0.099818434421927\n",
      "train loss:0.0695549386180149\n",
      "train loss:0.09710188449307931\n",
      "train loss:0.0443447165622406\n",
      "train loss:0.17815266110249803\n",
      "train loss:0.15699227407845653\n",
      "train loss:0.17230818716857765\n",
      "train loss:0.12216288653992781\n",
      "train loss:0.09227912949856329\n",
      "train loss:0.06735934096627594\n",
      "train loss:0.10216041244786604\n",
      "train loss:0.11912832086069983\n",
      "train loss:0.18028946840950294\n",
      "train loss:0.13991582479484496\n",
      "train loss:0.2450195955177643\n",
      "train loss:0.12353440941898602\n",
      "train loss:0.1527699859289679\n",
      "train loss:0.10468935105173932\n",
      "train loss:0.26146094137563525\n",
      "train loss:0.11852942366598923\n",
      "train loss:0.17095641502498754\n",
      "train loss:0.24349199900349025\n",
      "train loss:0.14512011952677104\n",
      "train loss:0.2259738665998977\n",
      "train loss:0.1822118986219425\n",
      "train loss:0.08805385880379597\n",
      "train loss:0.1220976374704167\n",
      "train loss:0.07410387670896698\n",
      "train loss:0.1275393260924096\n",
      "train loss:0.09843291096914536\n",
      "train loss:0.13339416619328695\n",
      "train loss:0.09962186651313934\n",
      "train loss:0.08318228253395821\n",
      "=== epoch:8, train acc:0.959, test acc:0.935 ===\n",
      "train loss:0.1760307372385944\n",
      "train loss:0.088664766575004\n",
      "train loss:0.20866858328160454\n",
      "train loss:0.11593834921695284\n",
      "train loss:0.0663723462994114\n",
      "train loss:0.2404606882723666\n",
      "train loss:0.1257757010389488\n",
      "train loss:0.12087360214250444\n",
      "train loss:0.09432460492891855\n",
      "train loss:0.04973762469791116\n",
      "train loss:0.1124751873855053\n",
      "train loss:0.06758328141803377\n",
      "train loss:0.12005449240122618\n",
      "train loss:0.07054327612370548\n",
      "train loss:0.07841858043100848\n",
      "train loss:0.14683653719593368\n",
      "train loss:0.10794076876917826\n",
      "train loss:0.07752514049941894\n",
      "train loss:0.15338448802797042\n",
      "train loss:0.08226911531401677\n",
      "train loss:0.03471813631875743\n",
      "train loss:0.1305040874614631\n",
      "train loss:0.10364515326287331\n",
      "train loss:0.06771791979435457\n",
      "train loss:0.11276675502592626\n",
      "train loss:0.09592307066428267\n",
      "train loss:0.11636261228732162\n",
      "train loss:0.06456479901907737\n",
      "train loss:0.07208044476417957\n",
      "train loss:0.07339022416597756\n",
      "train loss:0.12607968123224775\n",
      "train loss:0.06824991707707433\n",
      "train loss:0.06326883970832083\n",
      "train loss:0.13841137662888245\n",
      "train loss:0.13249752586363595\n",
      "train loss:0.05750698817473641\n",
      "train loss:0.09916435242619002\n",
      "train loss:0.07092816469190132\n",
      "train loss:0.13637659276394454\n",
      "train loss:0.07654084839772486\n",
      "train loss:0.2163983386319667\n",
      "train loss:0.12452879269617897\n",
      "train loss:0.16537911843040412\n",
      "train loss:0.07766949333952822\n",
      "train loss:0.12483116750016691\n",
      "train loss:0.20414030607229777\n",
      "train loss:0.11674873563520867\n",
      "train loss:0.04717719616801655\n",
      "train loss:0.09202184695058499\n",
      "train loss:0.1159581503079262\n",
      "=== epoch:9, train acc:0.96, test acc:0.936 ===\n",
      "train loss:0.042769226514541764\n",
      "train loss:0.09232576916166449\n",
      "train loss:0.059857951468119765\n",
      "train loss:0.08196864738573537\n",
      "train loss:0.0871319955905593\n",
      "train loss:0.12140784750986888\n",
      "train loss:0.040865109776372206\n",
      "train loss:0.06608060516509466\n",
      "train loss:0.06800681862100905\n",
      "train loss:0.09250077390851344\n",
      "train loss:0.07037526199971994\n",
      "train loss:0.1385420603262885\n",
      "train loss:0.08717687563927028\n",
      "train loss:0.06572878454991814\n",
      "train loss:0.02406281744527765\n",
      "train loss:0.0823429504974383\n",
      "train loss:0.12676740887540208\n",
      "train loss:0.10974938738943686\n",
      "train loss:0.07226633495909254\n",
      "train loss:0.06954520798004192\n",
      "train loss:0.11560066701084408\n",
      "train loss:0.12241934404108185\n",
      "train loss:0.07294408418684649\n",
      "train loss:0.04875895641312499\n",
      "train loss:0.08013205686987825\n",
      "train loss:0.08228404767129917\n",
      "train loss:0.11611191977172908\n",
      "train loss:0.06089283596522666\n",
      "train loss:0.11749351816976195\n",
      "train loss:0.1292939476793622\n",
      "train loss:0.06220417289668151\n",
      "train loss:0.09549432609371797\n",
      "train loss:0.066471811175666\n",
      "train loss:0.07682808852827724\n",
      "train loss:0.09212622659350231\n",
      "train loss:0.033330407725877434\n",
      "train loss:0.0966910633327827\n",
      "train loss:0.1483966902744619\n",
      "train loss:0.08963258845035435\n",
      "train loss:0.1049768113038319\n",
      "train loss:0.1146111704551383\n",
      "train loss:0.048377677022250126\n",
      "train loss:0.09279712550010019\n",
      "train loss:0.041027612674200055\n",
      "train loss:0.07328389932454378\n",
      "train loss:0.14516021583442368\n",
      "train loss:0.029616293937517354\n",
      "train loss:0.14460862654957335\n",
      "train loss:0.0675075999080323\n",
      "train loss:0.09391294900818084\n",
      "=== epoch:10, train acc:0.959, test acc:0.942 ===\n",
      "train loss:0.12638425455110724\n",
      "train loss:0.10985538832229792\n",
      "train loss:0.03556488798968812\n",
      "train loss:0.07133839357234704\n",
      "train loss:0.08109006445918762\n",
      "train loss:0.0557410203946051\n",
      "train loss:0.07536714582057376\n",
      "train loss:0.10619657445942739\n",
      "train loss:0.08166322756557108\n",
      "train loss:0.07526735248070514\n",
      "train loss:0.05946156187022148\n",
      "train loss:0.03369682206955903\n",
      "train loss:0.06680737754385743\n",
      "train loss:0.11937361723785234\n",
      "train loss:0.04258135051988979\n",
      "train loss:0.09558091203356277\n",
      "train loss:0.04315276396758302\n",
      "train loss:0.0396603437614767\n",
      "train loss:0.11327392520159255\n",
      "train loss:0.06901887898619989\n",
      "train loss:0.10227627098228\n",
      "train loss:0.11436009083177603\n",
      "train loss:0.06681647128661171\n",
      "train loss:0.09345424445842766\n",
      "train loss:0.07939479400383205\n",
      "train loss:0.07936512550311695\n",
      "train loss:0.1091904887529722\n",
      "train loss:0.09804839203020782\n",
      "train loss:0.09119492480719543\n",
      "train loss:0.09200471773927346\n",
      "train loss:0.08866520751692905\n",
      "train loss:0.04318794111031319\n",
      "train loss:0.09403454350324089\n",
      "train loss:0.1484816303309592\n",
      "train loss:0.2083308888131946\n",
      "train loss:0.03863820430019383\n",
      "train loss:0.1189521944135791\n",
      "train loss:0.06947614159448459\n",
      "train loss:0.03925044658518092\n",
      "train loss:0.14425947520506932\n",
      "train loss:0.09000413691717495\n",
      "train loss:0.03747145494301144\n",
      "train loss:0.08588661607278199\n",
      "train loss:0.11842229561775625\n",
      "train loss:0.11580411258811718\n",
      "train loss:0.052921955395511006\n",
      "train loss:0.06195386171659701\n",
      "train loss:0.09135937451872436\n",
      "train loss:0.06406640460105459\n",
      "train loss:0.09548872032913243\n",
      "=== epoch:11, train acc:0.971, test acc:0.949 ===\n",
      "train loss:0.038818620258393687\n",
      "train loss:0.1254302423988163\n",
      "train loss:0.0694741180393793\n",
      "train loss:0.0941548831134378\n",
      "train loss:0.0639351479074176\n",
      "train loss:0.04711108901379232\n",
      "train loss:0.115173639313271\n",
      "train loss:0.11690334756263335\n",
      "train loss:0.05718963728551426\n",
      "train loss:0.08125278632180923\n",
      "train loss:0.11626901599712962\n",
      "train loss:0.05848585348945875\n",
      "train loss:0.057539017382617505\n",
      "train loss:0.07445127880941144\n",
      "train loss:0.096040058499129\n",
      "train loss:0.054431055741558335\n",
      "train loss:0.11064632578957224\n",
      "train loss:0.04934908704749237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.08290194681668187\n",
      "train loss:0.05009882310391036\n",
      "train loss:0.09806288488268831\n",
      "train loss:0.11440998606940898\n",
      "train loss:0.11330774543707403\n",
      "train loss:0.04677255708973115\n",
      "train loss:0.08225155922326462\n",
      "train loss:0.05622654907199938\n",
      "train loss:0.037632446152879744\n",
      "train loss:0.08952561975865851\n",
      "train loss:0.13757575216809778\n",
      "train loss:0.048668054050983664\n",
      "train loss:0.037338781658087516\n",
      "train loss:0.045848262915719314\n",
      "train loss:0.03332823704119416\n",
      "train loss:0.04170026158932412\n",
      "train loss:0.03925176505729815\n",
      "train loss:0.03338189363098964\n",
      "train loss:0.05264515475309601\n",
      "train loss:0.04866259588286197\n",
      "train loss:0.07998663279646236\n",
      "train loss:0.05862103173486598\n",
      "train loss:0.11963537373069771\n",
      "train loss:0.05761316926448947\n",
      "train loss:0.04938575951096687\n",
      "train loss:0.045645488756810494\n",
      "train loss:0.066576225043297\n",
      "train loss:0.04702286126647581\n",
      "train loss:0.05878557153468842\n",
      "train loss:0.05600868678156066\n",
      "train loss:0.026120821125029812\n",
      "train loss:0.027676671316663327\n",
      "=== epoch:12, train acc:0.975, test acc:0.951 ===\n",
      "train loss:0.048923885990521374\n",
      "train loss:0.07520592957652522\n",
      "train loss:0.0872451328489252\n",
      "train loss:0.04638735551997653\n",
      "train loss:0.03993039866410802\n",
      "train loss:0.08388983292391902\n",
      "train loss:0.12535867968818712\n",
      "train loss:0.04853357622671413\n",
      "train loss:0.06169616482164294\n",
      "train loss:0.07837138055229051\n",
      "train loss:0.04506225805115353\n",
      "train loss:0.058857166438218894\n",
      "train loss:0.051135269889099645\n",
      "train loss:0.05273733859466317\n",
      "train loss:0.09269369588039275\n",
      "train loss:0.04765400152352321\n",
      "train loss:0.07242678713660354\n",
      "train loss:0.14214659773213653\n",
      "train loss:0.028025345497864698\n",
      "train loss:0.118154405596196\n",
      "train loss:0.07392122381639883\n",
      "train loss:0.027978189769516045\n",
      "train loss:0.035736041119736564\n",
      "train loss:0.06679077823032355\n",
      "train loss:0.053815144779612155\n",
      "train loss:0.05513762241116412\n",
      "train loss:0.07258434510213457\n",
      "train loss:0.05291259313933607\n",
      "train loss:0.060256600559517384\n",
      "train loss:0.0712566371655229\n",
      "train loss:0.05403382138975104\n",
      "train loss:0.037416884386074545\n",
      "train loss:0.024056574068952678\n",
      "train loss:0.046033534433675666\n",
      "train loss:0.02303731316816511\n",
      "train loss:0.061189783055061626\n",
      "train loss:0.07228971331742436\n",
      "train loss:0.0836586685742661\n",
      "train loss:0.08976680467082387\n",
      "train loss:0.05966730660670319\n",
      "train loss:0.04119640191107627\n",
      "train loss:0.08694084124948498\n",
      "train loss:0.033224028867650704\n",
      "train loss:0.05004812194614273\n",
      "train loss:0.05262776588360916\n",
      "train loss:0.06754159166256157\n",
      "train loss:0.024170233025095102\n",
      "train loss:0.06614494907966213\n",
      "train loss:0.02345520745596213\n",
      "train loss:0.04321135761550002\n",
      "=== epoch:13, train acc:0.984, test acc:0.956 ===\n",
      "train loss:0.039256796086077385\n",
      "train loss:0.035892732641818255\n",
      "train loss:0.04281716340652575\n",
      "train loss:0.06748631928165665\n",
      "train loss:0.0358328481974535\n",
      "train loss:0.06816234694716437\n",
      "train loss:0.052916427278117316\n",
      "train loss:0.04913735169404783\n",
      "train loss:0.03875035762550159\n",
      "train loss:0.03908653450572396\n",
      "train loss:0.054149014514496094\n",
      "train loss:0.02489452321621716\n",
      "train loss:0.06025566378554401\n",
      "train loss:0.01332533108257235\n",
      "train loss:0.03931198305261951\n",
      "train loss:0.09330926015898379\n",
      "train loss:0.03122603814243879\n",
      "train loss:0.012196143696455594\n",
      "train loss:0.056678981133090264\n",
      "train loss:0.04233284639939857\n",
      "train loss:0.04588324821784355\n",
      "train loss:0.023089350275633875\n",
      "train loss:0.09413505166523492\n",
      "train loss:0.12635333296147652\n",
      "train loss:0.04251735499221065\n",
      "train loss:0.033142520400684675\n",
      "train loss:0.03870235152998999\n",
      "train loss:0.02744305475419485\n",
      "train loss:0.0808863927131251\n",
      "train loss:0.024280346243802618\n",
      "train loss:0.03582593356795066\n",
      "train loss:0.01891392705568518\n",
      "train loss:0.04031147474408295\n",
      "train loss:0.025220932191123577\n",
      "train loss:0.02030674528087703\n",
      "train loss:0.07489749285437958\n",
      "train loss:0.08987744709697155\n",
      "train loss:0.02068359557415471\n",
      "train loss:0.016668969711664608\n",
      "train loss:0.05324014853488244\n",
      "train loss:0.05920633626580279\n",
      "train loss:0.060367670181927006\n",
      "train loss:0.022221989994043446\n",
      "train loss:0.046955843288590954\n",
      "train loss:0.023751631603975712\n",
      "train loss:0.02493160003548234\n",
      "train loss:0.04907627535266843\n",
      "train loss:0.04072516537117531\n",
      "train loss:0.027290620583415012\n",
      "train loss:0.12569230833663553\n",
      "=== epoch:14, train acc:0.981, test acc:0.961 ===\n",
      "train loss:0.031240068293350864\n",
      "train loss:0.05893962553254109\n",
      "train loss:0.04879638209564337\n",
      "train loss:0.08745553935372768\n",
      "train loss:0.028277933203405724\n",
      "train loss:0.028788054455381948\n",
      "train loss:0.020621652325957633\n",
      "train loss:0.05130073275672659\n",
      "train loss:0.03817813585061101\n",
      "train loss:0.01264206635945977\n",
      "train loss:0.010253402406095116\n",
      "train loss:0.017963260347034507\n",
      "train loss:0.05510678760722453\n",
      "train loss:0.07577532526246522\n",
      "train loss:0.045767150316708555\n",
      "train loss:0.07299528504856499\n",
      "train loss:0.04242499059221\n",
      "train loss:0.054095576207026604\n",
      "train loss:0.027581945926654066\n",
      "train loss:0.022148223893233388\n",
      "train loss:0.032241570075745546\n",
      "train loss:0.025676226878571008\n",
      "train loss:0.028537179792227262\n",
      "train loss:0.05776858153287812\n",
      "train loss:0.060932006654957085\n",
      "train loss:0.03227956152701483\n",
      "train loss:0.04236792281778005\n",
      "train loss:0.029486286420742733\n",
      "train loss:0.03840270700711031\n",
      "train loss:0.09239811741281935\n",
      "train loss:0.03144885367245663\n",
      "train loss:0.03091407787715163\n",
      "train loss:0.136614266338684\n",
      "train loss:0.03077846747210988\n",
      "train loss:0.03040505011696935\n",
      "train loss:0.06830391806796406\n",
      "train loss:0.03296445988458022\n",
      "train loss:0.05095184879688948\n",
      "train loss:0.03591327214237798\n",
      "train loss:0.05409092828019199\n",
      "train loss:0.022820455412743575\n",
      "train loss:0.03286566996607894\n",
      "train loss:0.017183588613127698\n",
      "train loss:0.033289187617306995\n",
      "train loss:0.09392147510439257\n",
      "train loss:0.03504756226194406\n",
      "train loss:0.04477611258471633\n",
      "train loss:0.0143511652338558\n",
      "train loss:0.037056071346847304\n",
      "train loss:0.03616172051901779\n",
      "=== epoch:15, train acc:0.984, test acc:0.958 ===\n",
      "train loss:0.01664692925126199\n",
      "train loss:0.030554469777557925\n",
      "train loss:0.07409286708197099\n",
      "train loss:0.033261034112524214\n",
      "train loss:0.03367719707744227\n",
      "train loss:0.07910844627620152\n",
      "train loss:0.02195859290106878\n",
      "train loss:0.03424959858087389\n",
      "train loss:0.058727662790448984\n",
      "train loss:0.06650571258084755\n",
      "train loss:0.013105138799244914\n",
      "train loss:0.01381835704172903\n",
      "train loss:0.0272110335337345\n",
      "train loss:0.03890757801064471\n",
      "train loss:0.05513711526000184\n",
      "train loss:0.03351913985469403\n",
      "train loss:0.035103596270074984\n",
      "train loss:0.016456903768734977\n",
      "train loss:0.033726901009583095\n",
      "train loss:0.036616158292669614\n",
      "train loss:0.02326467940489577\n",
      "train loss:0.01306444391177363\n",
      "train loss:0.083156301342416\n",
      "train loss:0.0795373278914178\n",
      "train loss:0.0461210236581012\n",
      "train loss:0.032579530288798\n",
      "train loss:0.016007345908726375\n",
      "train loss:0.017995378450172142\n",
      "train loss:0.0387473309594585\n",
      "train loss:0.015214781862436967\n",
      "train loss:0.03649192084922271\n",
      "train loss:0.030988422815282437\n",
      "train loss:0.057109353268325586\n",
      "train loss:0.03973032259884899\n",
      "train loss:0.038615781022728765\n",
      "train loss:0.0168029717828986\n",
      "train loss:0.03639033676664512\n",
      "train loss:0.0179187476626685\n",
      "train loss:0.034608724514997344\n",
      "train loss:0.03958277630090493\n",
      "train loss:0.017903338559627054\n",
      "train loss:0.014198302414836548\n",
      "train loss:0.014751094524146382\n",
      "train loss:0.03984188957548957\n",
      "train loss:0.015630038248627964\n",
      "train loss:0.00975843532059426\n",
      "train loss:0.016127228465767453\n",
      "train loss:0.049164661361644164\n",
      "train loss:0.017999678800265193\n",
      "train loss:0.04772747970367238\n",
      "=== epoch:16, train acc:0.986, test acc:0.958 ===\n",
      "train loss:0.04374325665236887\n",
      "train loss:0.038373051180411545\n",
      "train loss:0.03501716613461542\n",
      "train loss:0.03420210128777576\n",
      "train loss:0.06143069663819531\n",
      "train loss:0.05257545497205873\n",
      "train loss:0.01716536882790627\n",
      "train loss:0.08672908614821714\n",
      "train loss:0.019845123858351722\n",
      "train loss:0.025982044602861507\n",
      "train loss:0.022511454950545625\n",
      "train loss:0.015869093942933135\n",
      "train loss:0.010209159949499029\n",
      "train loss:0.012715766050480894\n",
      "train loss:0.009689471740860439\n",
      "train loss:0.04023290434842539\n",
      "train loss:0.08048790820526847\n",
      "train loss:0.03910798200267453\n",
      "train loss:0.015993510504262568\n",
      "train loss:0.020719318328899127\n",
      "train loss:0.03210375145141478\n",
      "train loss:0.023226878981901143\n",
      "train loss:0.016898632652327515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.014802586008718411\n",
      "train loss:0.024858588144863875\n",
      "train loss:0.01599301596998147\n",
      "train loss:0.01953746882414147\n",
      "train loss:0.024299984117041346\n",
      "train loss:0.016082162015016242\n",
      "train loss:0.016804603926348444\n",
      "train loss:0.0377649834585229\n",
      "train loss:0.05224473401717547\n",
      "train loss:0.045207925811232744\n",
      "train loss:0.01162217915529538\n",
      "train loss:0.027628866251470832\n",
      "train loss:0.025817927404494193\n",
      "train loss:0.01339191051954331\n",
      "train loss:0.01598788160868174\n",
      "train loss:0.044828569559539996\n",
      "train loss:0.03565258837462147\n",
      "train loss:0.010663736687191519\n",
      "train loss:0.012376817997982362\n",
      "train loss:0.01221851196469121\n",
      "train loss:0.014763291143556482\n",
      "train loss:0.0633099344795378\n",
      "train loss:0.03960672286782302\n",
      "train loss:0.035015999286535\n",
      "train loss:0.024057766478847324\n",
      "train loss:0.06858436520187873\n",
      "train loss:0.022846595943334814\n",
      "=== epoch:17, train acc:0.987, test acc:0.955 ===\n",
      "train loss:0.027327651008266778\n",
      "train loss:0.029167213757280378\n",
      "train loss:0.02712158207372076\n",
      "train loss:0.047843196001329726\n",
      "train loss:0.027230848870121668\n",
      "train loss:0.029259446125500747\n",
      "train loss:0.025468219743198958\n",
      "train loss:0.008419421793970821\n",
      "train loss:0.02303577596224057\n",
      "train loss:0.02158382547543636\n",
      "train loss:0.021783573558205673\n",
      "train loss:0.01788823533957614\n",
      "train loss:0.010887406198371283\n",
      "train loss:0.03351746118701219\n",
      "train loss:0.035904596492923775\n",
      "train loss:0.03988732154366792\n",
      "train loss:0.012551445048532615\n",
      "train loss:0.03969562395953771\n",
      "train loss:0.019539711919066904\n",
      "train loss:0.029193023538404802\n",
      "train loss:0.006278370924140189\n",
      "train loss:0.021957049010810706\n",
      "train loss:0.03294049404947916\n",
      "train loss:0.02276419883080376\n",
      "train loss:0.02396088924534444\n",
      "train loss:0.014306175645189139\n",
      "train loss:0.05364559781813611\n",
      "train loss:0.021321554384101563\n",
      "train loss:0.008980737453696054\n",
      "train loss:0.011776954612870092\n",
      "train loss:0.017881902846640593\n",
      "train loss:0.015433999464765726\n",
      "train loss:0.05007599885886726\n",
      "train loss:0.07519831869191902\n",
      "train loss:0.035977719199687105\n",
      "train loss:0.02168805312502233\n",
      "train loss:0.02393899965916316\n",
      "train loss:0.023134131538179593\n",
      "train loss:0.02886076878544188\n",
      "train loss:0.02475810054006417\n",
      "train loss:0.020841495359042936\n",
      "train loss:0.02360783471617662\n",
      "train loss:0.030240615179897325\n",
      "train loss:0.020282916969539894\n",
      "train loss:0.02567503077056216\n",
      "train loss:0.04777369671635059\n",
      "train loss:0.020330369778903238\n",
      "train loss:0.029981845311531065\n",
      "train loss:0.01598798636631193\n",
      "train loss:0.024420974025592273\n",
      "=== epoch:18, train acc:0.99, test acc:0.956 ===\n",
      "train loss:0.03039326382194843\n",
      "train loss:0.015431022253183895\n",
      "train loss:0.020036791396454326\n",
      "train loss:0.042919850193482253\n",
      "train loss:0.056666518538675215\n",
      "train loss:0.028710108217239058\n",
      "train loss:0.027493817352425023\n",
      "train loss:0.025033220303257005\n",
      "train loss:0.01826049327632367\n",
      "train loss:0.014798839465153354\n",
      "train loss:0.05055157149419234\n",
      "train loss:0.016453319495794552\n",
      "train loss:0.02041078721029542\n",
      "train loss:0.016007522674335604\n",
      "train loss:0.01723067403725015\n",
      "train loss:0.013002894146143307\n",
      "train loss:0.03183706245487169\n",
      "train loss:0.04134935778762506\n",
      "train loss:0.02968705077413308\n",
      "train loss:0.014182816678651724\n",
      "train loss:0.020501789401089057\n",
      "train loss:0.011597366106618217\n",
      "train loss:0.02288534845897589\n",
      "train loss:0.015504639903520112\n",
      "train loss:0.034987546926964236\n",
      "train loss:0.01462554818666063\n",
      "train loss:0.02078883351311928\n",
      "train loss:0.01927608435047506\n",
      "train loss:0.020528570112293767\n",
      "train loss:0.051844024743682694\n",
      "train loss:0.017382537813135136\n",
      "train loss:0.013447227251932392\n",
      "train loss:0.01538143911204354\n",
      "train loss:0.025826268871445134\n",
      "train loss:0.013806354660069776\n",
      "train loss:0.01115504032668588\n",
      "train loss:0.019361719938816083\n",
      "train loss:0.026055098203943423\n",
      "train loss:0.033933571854023854\n",
      "train loss:0.03211536270526076\n",
      "train loss:0.0163013669303872\n",
      "train loss:0.012561005073380616\n",
      "train loss:0.016789532077109204\n",
      "train loss:0.013461698136303274\n",
      "train loss:0.029383621361617242\n",
      "train loss:0.07904084112544499\n",
      "train loss:0.01106294135248099\n",
      "train loss:0.015813864041582578\n",
      "train loss:0.024137223839897918\n",
      "train loss:0.020343434782487874\n",
      "=== epoch:19, train acc:0.992, test acc:0.964 ===\n",
      "train loss:0.05031716310332098\n",
      "train loss:0.0090713645688733\n",
      "train loss:0.016323700646925194\n",
      "train loss:0.021015378821945405\n",
      "train loss:0.03358561012410875\n",
      "train loss:0.033809427533552414\n",
      "train loss:0.010574994523008465\n",
      "train loss:0.03444317632769104\n",
      "train loss:0.008742523107991134\n",
      "train loss:0.022287148083671143\n",
      "train loss:0.010398271827433657\n",
      "train loss:0.02107851547542488\n",
      "train loss:0.019341786754745627\n",
      "train loss:0.019481073590578615\n",
      "train loss:0.023177988299298864\n",
      "train loss:0.01851662188063482\n",
      "train loss:0.03450512636848237\n",
      "train loss:0.02189248313561707\n",
      "train loss:0.030450231587958947\n",
      "train loss:0.005923893102132669\n",
      "train loss:0.035441449204904225\n",
      "train loss:0.007693102912713514\n",
      "train loss:0.026515653003661595\n",
      "train loss:0.016967127118993477\n",
      "train loss:0.01627221192364914\n",
      "train loss:0.015837986696728378\n",
      "train loss:0.005889168957633505\n",
      "train loss:0.01407185946434781\n",
      "train loss:0.019527402202960863\n",
      "train loss:0.014013159611807359\n",
      "train loss:0.0036859678716193557\n",
      "train loss:0.031928441474340256\n",
      "train loss:0.013118517259605849\n",
      "train loss:0.015252907125183995\n",
      "train loss:0.01241297275486792\n",
      "train loss:0.014557707944071196\n",
      "train loss:0.017366772801330966\n",
      "train loss:0.012872453638867005\n",
      "train loss:0.009052856417910041\n",
      "train loss:0.005820798425156046\n",
      "train loss:0.006585594138138633\n",
      "train loss:0.03405619874825995\n",
      "train loss:0.017769737189583867\n",
      "train loss:0.020038583615511314\n",
      "train loss:0.027592912505725688\n",
      "train loss:0.02513539554522632\n",
      "train loss:0.011557299420521326\n",
      "train loss:0.013795172407702156\n",
      "train loss:0.019419724683418495\n",
      "train loss:0.02232747564911985\n",
      "=== epoch:20, train acc:0.991, test acc:0.961 ===\n",
      "train loss:0.010473835932626255\n",
      "train loss:0.06593213539295548\n",
      "train loss:0.008425108641191503\n",
      "train loss:0.012891651311220552\n",
      "train loss:0.007549106271614884\n",
      "train loss:0.00719607383566252\n",
      "train loss:0.010632051750245486\n",
      "train loss:0.009962621182863101\n",
      "train loss:0.01616966314248178\n",
      "train loss:0.0372233230991975\n",
      "train loss:0.01618575298109803\n",
      "train loss:0.012364101149415902\n",
      "train loss:0.007924455934373194\n",
      "train loss:0.011702573499773765\n",
      "train loss:0.008699473943076595\n",
      "train loss:0.013204976260738742\n",
      "train loss:0.01087971818755658\n",
      "train loss:0.02005024111670504\n",
      "train loss:0.007011469068560799\n",
      "train loss:0.008989188293158205\n",
      "train loss:0.008258455264960498\n",
      "train loss:0.006423518225568465\n",
      "train loss:0.008102080787205389\n",
      "train loss:0.007064213310762528\n",
      "train loss:0.014796319206769003\n",
      "train loss:0.010300055554116939\n",
      "train loss:0.02074788328896714\n",
      "train loss:0.015945731917845474\n",
      "train loss:0.006081813484684701\n",
      "train loss:0.006819602112790304\n",
      "train loss:0.009034031323377056\n",
      "train loss:0.008900350646224738\n",
      "train loss:0.0038764091943887867\n",
      "train loss:0.02093353213354391\n",
      "train loss:0.014591246547665809\n",
      "train loss:0.004488726247769292\n",
      "train loss:0.027541142141887288\n",
      "train loss:0.008352281414179223\n",
      "train loss:0.008958827714476886\n",
      "train loss:0.01466167707657392\n",
      "train loss:0.00678762323313483\n",
      "train loss:0.005269385656295039\n",
      "train loss:0.0032737311072752813\n",
      "train loss:0.0077841101987080755\n",
      "train loss:0.013071900685583074\n",
      "train loss:0.01091637200868898\n",
      "train loss:0.026612264848099164\n",
      "train loss:0.010917697325712643\n",
      "train loss:0.011154496102639267\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.962\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e208d1",
   "metadata": {},
   "source": [
    "대표적인 CNN으로는 LeNET와 AlexNet이 있습니다. \n",
    "\n",
    "Summary \n",
    "- CNN은 지금까지의 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가한다.\n",
    "- 합성곱 계층과 풀링 계층은 im2col (이미지를 행렬로 전개하는 함수)을 이용하면 간단하고 효율적으로 구현할 수 있다.\n",
    "- CNN을 시각화해보면 계층이 깊어질수록 고급 정보가 추출되는 모습을 확인할 수 있다.\n",
    "- 대표적인 CNN에는 LeNet과 AlexNet이 있다.\n",
    "- 딥러닝의 발전에는 빅 데이터와 GPU가 크게 기여했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183402e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
