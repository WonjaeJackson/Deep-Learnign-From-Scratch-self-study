{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109b27ce",
   "metadata": {},
   "source": [
    "# Chapter 4 신경망 학습\n",
    "\n",
    "학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 뜻합니다. 이번 Chapter에서는 신경망이 학습할 수 있도록 해주는 지표인 손실 함수를 소개합니다.\n",
    "\n",
    "신경망의 특징은 데이터를 보고 학습할 수 있다는 점입니다. \n",
    "\n",
    "기계학습은 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만듭니다. 그래서 항상 기계학습의 중심에는 데이터가 존재합니다. 이처럼 데이터가 이끄는 접근 방식 덕에 사람 중심 접근에서 벗어날 수 있습니다. 하지만 사람이 학습에 필요한 특징을 사람이 설계합니다. \n",
    "\n",
    "딥러닝(aka end to end machine learning)은 기계학습과 다르게 있는 그대로 학습합니다. \n",
    "\n",
    "기계학습 문제는 데이터를 훈련 데이터와 시험 데이터로 나눠 학습과 실험을 수행합니다. 훈련 데이터에서 최적의 매개변수를 찾고 시험 테이터를 사용하여 훈란한 모델의 실력을 평가하는 것입니다. 데이터를 두 개로 나누는 이유는 범용성을 평가하기 위해서입니다. 훈련 데이터로 학습한 기계학습을 시험 데이터로 평가해 범용 능력을 확인하기 위해서 입니다. \n",
    "\n",
    "또한 한 데이터셋에만 지나치게 최적화된 상태를 오버피팅이라고 합니다. \n",
    "\n",
    "손실 함수 \n",
    "- 신경망 학습에서는 현재의 상태를 하나의 지표로 표현합니다. 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것입니다. \n",
    "- 신경망에서는 이 함수를 손실 함수라고 합니다. \n",
    "- 일반적으로는 오차제곱합과 교차 엔트로피 오차를 사용합니다. \n",
    "\n",
    "오차제곱합 \n",
    "\n",
    "$$ E = \\frac{1}{2} \\sum \\limits _{k} (y_{k} - t_{k})^2$$ \n",
    "\n",
    "여기서 yk는 신경망의 출력, tk는 정답 레이블, k는 데이터의 차원 수를 나타냅니다. 이 배열들의 원소는 첫 번째 인덱스부터 순서대로 숫자 0, 1, 2 ...일 때의 값입니다. 여기에서 신경망의 출력 y는 소프트맥스 함수의 출력입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30b08cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "## 오차제곱합 \n",
    "import numpy as np\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t) **2)\n",
    "\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(sum_squares_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d28558",
   "metadata": {},
   "source": [
    "첫 번재 결과값에서 손실함수의 값이 작으니 춧 번째 추정 결과가 정답에 더 가까울 것으로 판단할 수 있습니다.\n",
    "\n",
    "교차 엔트로피 오차 \n",
    "\n",
    "$$ E = \\sum \\limits _{k} t_{k} \\log y_{k}$$ \n",
    "\n",
    "여기서 yk는 신경망의 출력, tk는 정답 레이블입니다. 또 tk는 정답에 해당하는 인덱스의 원소만 1이고 1이고 나머지는 0입니다. (원-핫 인코딩). 그래서는 실질적으로 정답일 때의 추정(tk가 1일 때의 yk)의 자연로그를 계산하는 식이 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a684e330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "## cross_entropy 구현 \n",
    "def cross_entropy_error(y, t): \n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))\n",
    "\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1b3b8",
   "metadata": {},
   "source": [
    "오차제곱합과 같이 첫 번째 entropy 값이 훨씬 작기 때문에 오차제곱합과 엔트로피의 판단이 일치합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336f7a7",
   "metadata": {},
   "source": [
    "미니배치 학습\n",
    "\n",
    "지금까지 데이터 하나에 대한 손실 함수만 생각해왔으니, 이제 훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각 할 차례입니다.\n",
    "\n",
    "교차 엔트로피 오차의 공식을 다수의 데이터로 생각했을 때 아래의 공식이 나옵니다.\n",
    "\n",
    "$$E = - \\frac{|1}{N} \\sum \\limits _{n} \\sum \\limits _{k} t_{nk} \\log y_{nk}$$ \n",
    "\n",
    "이때 데이터가 N개라면 tnk는 n번째 데이터의 k번째 값을 의미합니다 (ynk는 신경망의 출력 tk는 정답 레이블입니다).  \n",
    "\n",
    "하지만 모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 많이 소요됩니다. 그래서 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행하며 이 일부를 mini-batch라고 합니다. 예를 들어 10000 장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만 사용하여 학습하는 것을 미니배치 학습이라고 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00093a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([19033, 46706, 48548,   209, 23940, 15663, 26071, 31871, 24416,\n",
       "       32378])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## mini-batch \n",
    "import sys, os \n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np \n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(one_hot_label=True, normalize=True)\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10 \n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "## or \n",
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3e06d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross_entropy \n",
    "## one hot encoding \n",
    "## since only one element equals one while rest equals zero, ignore the computation \n",
    "def cross_entropy_error(y, t): \n",
    "    if y.ndim == 1: \n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881ec9a",
   "metadata": {},
   "source": [
    "신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾ㅅ습니다. 이때 매개변수의 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복합니다. 가령 가상의 신경망이 있고 그 신경망의 어느 한 가중치 매개변수에 주목한다고 합시다. 이때 그 가중치 매개변수의 손실 함수의 미분이란 가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나라는 의미입니다. 만약 이 미분 값이 음수면 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있고 만약 양수면 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있습니다. 그러나 미분 값이 0이면 가중치 매개변수의 갱신이 멈춥니다. \n",
    "\n",
    "정확도를 지표로 삼아서는 안 되는 이유는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문입니다. \n",
    "\n",
    "만약 정확도를 지표로 삼으면 가중치 매개변수의 값을 조금 바꾼다고 해도 정확도는 그대로 32%로일 것입니다. 즉 매개변수를 약간만 조정해서는 정확도가 개선되지 않고 일정하게 유지됩니다. 혹, 정확도가 개선된다 하더라도그 값은 연속적인 변화보다는 33%나 34%처럼 불연속적인 띄엄띄엄한 값으로 바뀌어버립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02154402",
   "metadata": {},
   "source": [
    "수치 미분 \n",
    "- 수치 미분은 작은 차분으로 미분하는 것으로 근사값을 구하는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e50128b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 수치 미분 예제 \n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2 * h)\n",
    "def function_1(x): \n",
    "    return 0.01 * x**2 + 0.1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d2cc74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAiWUlEQVR4nO3deXhV1b3/8feXhBAIcwbmAGGSQcZAglKqOFzlUlGrFixSlUGtVu291uut/Vlbe68d1OvUWlFQkNEJBxxxlgqBAGEM8xSmDIwJgYQk6/dHwr2YJiFAdvY5J5/X8+Th5Ox9sr6uc/JxZ++11zLnHCIiEnrq+V2AiIh4QwEvIhKiFPAiIiFKAS8iEqIU8CIiISrc7wJOFxMT4zp16uR3GSIiQWP58uU5zrnYirYFVMB36tSJ1NRUv8sQEQkaZrazsm06RSMiEqIU8CIiIUoBLyISojwNeDNrbmZvmtkGM0s3s6FeticiIv/H64uszwAfO+duMLMIoJHH7YmISBnPAt7MmgLDgVsBnHOFQKFX7YmIyPd5eYomAcgGXjGzlWb2splFedieiIicxsuADwcGAi845wYAx4CHyu9kZpPNLNXMUrOzsz0sR0Qk8CzfeZCXvtnmyc/2MuB3A7udcyll379JaeB/j3NuinMu0TmXGBtb4c1YIiIhKX3fUW57ZRmzUnZyrKCoxn++ZwHvnNsPZJhZj7KnLgPWe9WeiEgw2ZFzjFumLqVRRDivTUgiqkHNXxL1ehTNL4BZZSNotgG3edyeiEjA23/kBOOmplBcUsLcyUPp0NKbAYaeBrxzLg1I9LINEZFgcji/kPHTUjh0rJA5k5PpGtfEs7YCarIxEZFQdqygiFtfWcaOA/m8ettg+rZv7ml7mqpARKQWnDhZzMTpqazZc4Tnxw7goi4xnrepgBcR8VhhUQk/n7WCJdsP8OSN/biyd+taaVcBLyLioeISxy/npfHFhiz+69oLuXZAu1prWwEvIuKRkhLHf7y1mg/W7OPhkT25OSm+VttXwIuIeMA5x+/eX8eby3dz32XdmDQ8odZrUMCLiHjgL59sZPrinUwc1pn7L+/mSw0KeBGRGvbXL7fwt6+2MnZIPA//a0/MzJc6FPAiIjXo1X9s5y+fbGR0/7b84do+voU7KOBFRGrM66kZPPr+eq7o1YonbuxHWD3/wh0U8CIiNWLB6r089NZqftAthudvHkD9MP/j1f8KRESC3BcbMrl/bhqDOrbgxVsG0SA8zO+SAAW8iMh5+XZzNnfOXEHPNk2ZeutgGkUEzhRfCngRkXP03dYcJk5PJSEmihm3D6FpZH2/S/oeBbyIyDlYuv0gE15NJb5lI2ZNTKJFVITfJf0TBbyIyFlavvMQt72ylDbNI5k1KYnoxg38LqlCCngRkbOwKuMwt05bSmyTBsyZlExck0i/S6qUAl5EpJrW7jnCLVNTaB5Vn9mTkmnVNHDDHRTwIiLVkr7vKOOmptAksj6zJybTtnlDv0s6IwW8iMgZbM7MZdzLKUSGhzF7UpJni2TXNAW8iEgVtmbnMfalFOrVM2ZPSqJjdJTfJVWbAl5EpBI7co5x80tLAMecSUkkxDb2u6SzooAXEalAxsF8bn5pCYVFJcyamEzXuCZ+l3TWAueeWhGRAJFxMJ8xU5ZwrLCY2ZOS6NE6+MIdFPAiIt+z60A+Y6Ys5lhhMbMmJtG7bTO/Szpnnga8me0AcoFioMg5l+hleyIi52PngWOMnbKE/JOl4d6nXfCGO9TOEfylzrmcWmhHROSc7cg5xtiXlnDiZDGzJybTq21Tv0s6bzpFIyJ13vac0iP3wuISZk9Kpmeb4A938H4UjQM+NbPlZja5oh3MbLKZpZpZanZ2tsfliIh837bsPMZMWVwW7kkhE+7gfcBf7JwbCFwN3G1mw8vv4Jyb4pxLdM4lxsbGelyOiMj/2Zqdx5gpSygqdsyZlMwFrUMn3MHjgHfO7S37NwuYDwzxsj0RkeraklUa7iXOMWdyctAOhayKZwFvZlFm1uTUY+BKYK1X7YmIVNeWrFzGTFmCczBnUjLdW4VeuIO3F1lbAfPN7FQ7s51zH3vYnojIGW3OzGXsS0swM+ZMSqZrXHBNP3A2PAt459w2oJ9XP19E5Gxt3J/LT1+uG+EOmotGROqItXuO8JMpiwmrZ8ydHPrhDgp4EakDlu88xNiXlhAVEc7rdwylS5DNCnmudKOTiIS0xVsPMGH6MuKaNGDWpGTaBcFKTDVFAS8iIevrTdlMnpFKfMtGzJqYRFyAr6Fa0xTwIhKSFq7P5O5ZK+gS15iZE4YQ3biB3yXVOgW8iIScBav3cv/cNHq3a8aM24bQrFF9v0vyhS6yikhIeWv5bu6ds5IB8c2ZOaHuhjvoCF5EQsislJ08PH8tF3eN5qXxiTSKqNsRV7f/60UkZExdtJ3HFqxnxAVx/O2nA4msH+Z3Sb5TwItI0Pvrl1v4yycbubpPa54ZM4CIcJ19BgW8iAQx5xx//HgDL369jWv7t+WJG/sRHqZwP0UBLyJBqbjE8Zt31jBnaQbjkuP5/TV9qFfP/C4roCjgRSToFBaV8MvX0/hg9T7uvrQLD1zZg7KZa+U0CngRCSrHC4u5c+Zyvt6Uza9HXsDk4V38LilgKeBFJGgcOX6SCa8uY8WuQ/zpxxfyk8HxfpcU0BTwIhIUsnMLGD9tKVuycnn+5oGMvLCN3yUFPAW8iAS83YfyGfdyCplHC5j6s8EM7x7rd0lBQQEvIgFtS1Yu415eSn5hETMnJjGoYwu/SwoaCngRCVirdx/mZ9OWElavHvPuGErPNk39LimoKOBFJCAt2XaAidNTad6oPjMnJNEpJsrvkoKOAl5EAs5Ha/Zx37w0OrZsxGsTkmjdrG4t1FFTFPAiElBeW7KTR95dy4AOzZl262CaN4rwu6SgpYAXkYDgnOOphZt47ostXN4zjufGDqRhhGaEPB8KeBHxXVFxCb95Zy1zl2Xwk8QO/Nd1fTRpWA3wPODNLAxIBfY450Z53Z6IBJfjhcX8Ys5KPkvP5BcjuvJvV3TXvDI1pDaO4O8D0gGNbxKR7zmcX8iE6ams2HWIx0b35pahnfwuKaR4+jeQmbUH/hV42ct2RCT47D18nBv+vpg1u4/wt5sHKtw94PUR/NPAg0CTynYws8nAZID4eE0cJFIXbMrMZfzUpRwrKGLGhCEkJ0T7XVJI8uwI3sxGAVnOueVV7eecm+KcS3TOJcbGan4JkVC3bMdBbnjhO0qc4/U7hyrcPeTlEfzFwDVmNhKIBJqa2Uzn3DgP2xSRAPbx2v3cN3cl7Vo0ZMbtQ2jfopHfJYU0z47gnXP/6Zxr75zrBIwBvlC4i9RdUxdt565Zy+nVtilv3nmRwr0WaBy8iHiquMTx2IL1vPrdDq7q3Zqnx/Qnsr5uYKoNtRLwzrmvgK9qoy0RCRzHC4u5d+5KFq7PZMKwzvx6ZE/CtDB2rdERvIh4Iju3gInTl7F6zxEe/VEvbr24s98l1TkKeBGpcVuz87j1laVk5xbw4rhBXNm7td8l1UkKeBGpUUu3H2TSjFTqhxlzJw+lf4fmfpdUZyngRaTGvLdqLw+8vor2LRvy6q1DiI/WSBk/KeBF5Lw553jh6638+eONDOnckim3DNI87gFAAS8i5+VkcQmPvLuOOUt3cU2/tvzlxr40CNcwyECggBeRc3Yk/yR3z17Boi053HVJF351ZQ/qaRhkwFDAi8g52ZFzjNunLyPjYD5/vqEvNyV28LskKUcBLyJnbfHWA9w1q3QewZkTkkjShGEBSQEvImdl3rJdPDx/LR2jGzHt1sF0jI7yuySphAJeRKqluMTxp483MOWbbfygWwzP3zyQZg3r+12WVEEBLyJnlFdQxP1zV/JZehbjh3bkkVG9tCh2EFDAi0iV9hw+zoRXl7E5K4/fj+7NeC2tFzQU8CJSqRW7DjF5xnIKThbzyq2DGd5dq64FEwW8iFTo3bQ9/OrN1bRuGsmcSUl0a1Xp0soSoBTwIvI9xSWOv3yykb9/vZUhnVry91sG0TJK0w4EIwW8iPyvI8dPct/clXy1MZubk+J59Ee9iQjXxdRgpYAXEQC2ZOUxaUYqGQfz+cO1fRiX3NHvkuQ8KeBFhM/TM7l/bhoR4fWYPSmZIZ1b+l2S1AAFvEgd5pzjb19t5YlPN9K7bVNevCWRds0b+l2W1BAFvEgdlV9YxK/eWM0Ha/Yxun9b/nh9XxpGaJrfUKKAF6mDMg7mM2lGKpsyc/n1yAuY9IMEzDTNb6hRwIvUMd9tzeHuWSsoLnG8ctsQfqibl0JWtQLezOKAi4G2wHFgLZDqnCvxsDYRqUHOOV75xw7+68N0OsdE8dL4RDrHaCbIUFZlwJvZpcBDQEtgJZAFRALXAl3M7E3gSefc0QpeGwl8AzQoa+dN59xva7R6EamWYwVFPPT2Gt5ftZcrerXiqZv60SRSM0GGujMdwY8EJjnndpXfYGbhwCjgCuCtCl5bAIxwzuWZWX1gkZl95Jxbcr5Fi0j1bc3O487XlrM1O48Hr+rBncO7aFm9OqLKgHfO/aqKbUXAO1Vsd0Be2bf1y77c2ZcoIufq47X7eeCNVUSE1+O1CUlc3DXG75KkFlXrHmQze83Mmp32fScz+7warwszszRKT+0sdM6lVLDPZDNLNbPU7OzssyhdRCpTVFzC4x+lc+fM5XSJa8yCXwxTuNdB1Z1kYhGQYmYjzWwS8Cnw9Jle5Jwrds71B9oDQ8ysTwX7THHOJTrnEmNjdTVf5Hzl5BVwy9SlvPj1NsYlx/P6Hcm01c1LdVK1RtE45140s3XAl0AOMMA5t7+6jTjnDpvZV8BVlI7AEREPrNh1iJ/PXMGh/EKeuLEfNwxq73dJ4qPqnqK5BZgGjAdeBT40s35neE2smTUve9wQuBzYcD7FikjFnHPMWLyDn7y4mPrhxts/v0jhLtW+0enHwDDnXBYwx8zmUxr0A6p4TRtgupmFUfo/ktedcwvOp1gR+Wf5hUX8Zv5a3l65hxEXxPE/N/WnWSMNgZTqn6K5ttz3S80s6QyvWU3V/wMQkfO0OTOXn89awZbsPP7tiu7cc2lXDYGU/1XlKRoz+42ZVThvqHOu0MxGmNkob0oTkaq8tXw31zz/Dw7lF/La7Unce1k3hbt8z5mO4NcA75vZCWAFkE3pnazdgP7AZ8B/e1mgiHzf8cJiHnl3LW8s301yQkueHTOAuKaRfpclAehMAX+Dc+5iM3uQ0rHsbYCjwExgsnPuuNcFisj/2ZJVekpmc1Ye947oyn2XdydMR+1SiTMF/CAz6wj8FLi03LaGlE48JiK14O0Vu3l4/loaRYQx4/Yh/KCb7huRqp0p4P8OfAwkAKmnPW+UTjuQ4FFdIlLmeGExj763jnmpGSR1bsmzYwfQSqdkpBrONBfNs8CzZvaCc+6uWqpJRMpsycrl7lkr2ZSVyy9GdOW+y7oRHlbdG9ClrqvuMEmFu0gtcs4xb1kGj76/jqiIcKbfNoThWphDzpJWdBIJMEeOn+TXb6/hgzX7GNY1hqdu6qdRMnJOFPAiASR1x0Hum5tG5tETPHT1BUz+QYLGtss5U8CLBIDiEsdfv9zC059tokPLRrx510X079Dc77IkyCngRXy29/Bx7p+XxtLtB7luQDt+P7q3ltOTGqGAF/HRx2v38x9vraaouISnburH9QM1A6TUHAW8iA/yC4v4wwfpzE7ZxYXtmvHs2AF0jonyuywJMQp4kVqWlnGYX85LY8eBY9wxPIF/v7IHEeEa2y41TwEvUkuKikt4/sstPPfFFlo3jWTOpGSSE6L9LktCmAJepBZszznG/fPSWJVxmOsGtON3o3vTVBdSxWMKeBEPOeeYszSDxxasJyK8Hs/fPIBRfdv6XZbUEQp4EY9k5xbw0Fur+XxDFsO6xvDEjf1o3Ux3pErtUcCLeGDh+kweems1uQVFPDKqF7de1El3pEqtU8CL1KAj+Sf53YJ1vL1iDz3bNGXOmP50b9XE77KkjlLAi9SQLzdm8dBbq8nJK+TeEV25Z0Q3DX8UXyngRc5T7omT/GFBOvNSM+gW15iXxifSt31zv8sSUcCLnI9Fm3N48M1V7D96gjt/2IX7L+9GZP0wv8sSARTwIufkWEERj3+Uzswlu0iIjeLNuy5iYHwLv8sS+R7PAt7MOgAzgNZACTDFOfeMV+2J1JYl2w7wqzdXsfvQcSYO68wD/9JDR+0SkLw8gi8C/t05t8LMmgDLzWyhc269h22KeCb3xEn++NEGZqXsomN0I16/YyiDO7X0uyyRSnkW8M65fcC+sse5ZpYOtAMU8BJ0Pk/P5DfvrCXz6AkmDuvMv13ZnUYROsMpga1WPqFm1gkYAKRUsG0yMBkgPj6+NsoRqbYDeQX87v31vLdqLz1aNeGFcYO00pIEDc8D3swaA28B9zvnjpbf7pybAkwBSExMdF7XI1IdzjneTdvL795fR15BEb+8vDt3XdJF49olqHga8GZWn9Jwn+Wce9vLtkRqyt7Dx3l4/hq+3JjNgPjm/OnHfXU3qgQlL0fRGDAVSHfOPeVVOyI1paTEMStlJ3/8aAMlDh4Z1YufXdSJMM0hI0HKyyP4i4FbgDVmllb23K+dcx962KbIOUnfd5Rfz1/Dyl2HGdY1hsevv5AOLRv5XZbIefFyFM0iQIc+EtDyC4t4+rPNTF20neYN6/PUTf24bkA7Sv8AFQluGuclddZn6zP57Xvr2HP4OGMGd+Chqy+geaMIv8sSqTEKeKlz9h05zqPvreOTdZl0b9WYN+7UDUsSmhTwUmcUFZcwffFOnvp0I8XO8eBVPZg4LEFDHyVkKeClTli56xD/7921rN1zlEt6xPLY6D66iCohTwEvIe1AXgF/+ngDr6fuJq5JA/5680BGXthaF1GlTlDAS0gqKi5hVsounvx0I/mFxdwxPIFfXNaNxg30kZe6Q592CTnLdhzkkXfXkb7vKMO6xvDoNb3pGtfY77JEap0CXkJG1tETPP7RBuav3EPbZpG88NOBXNVHp2Ok7lLAS9A7WVzC9O928PRnmyksKuGeS7vy80u7aDpfqfP0GyBByznHlxuz+MMH6WzLPsYlPWL57Y960zkmyu/SRAKCAl6C0qbMXB5bsJ5vN+eQEBPFy+MTuaxnnE7HiJxGAS9B5eCxQv5n4SZmL91FVEQY/29UL25J7qiblUQqoICXoFBYVMKMxTt45vPN5BcWMy4pnvsv706LKM0dI1IZBbwENOccC9dn8t8fprPjQD6X9Ijl4ZE96aYFOETOSAEvAWtVxmEe/yidJdsO0jWuMa/cNphLe8T5XZZI0FDAS8DZeeAYf/5kIx+s3kd0VAS/H92bsUPiqR+m8+wiZ0MBLwEjJ6+A5z7fzKyUXdQPq8e9I7oyaXgCTSLr+12aSFBSwIvv8guLePnb7Uz5ZhvHTxbzk8EduP+ybsQ1jfS7NJGgpoAX3xQVlzAvNYOnP9tMdm4B/9K7FQ9edQFdYjVvjEhNUMBLrSspcXywZh//89kmtmUfI7FjC/4+biCDOmpVJZGapICXWnNqyONTCzexYX8u3Vs1Zsotg7iiVyvdgSriAQW8eM45x7ebc3jy042s2n2EzjFRPDOmP6P6tiWsnoJdxCsKePFUyrYDPPnpJpbuOEi75g358w19uX5AO8I15FHEcwp48URaxmGe/HQj327OIa5JAx4b3ZubBnegQXiY36WJ1BkKeKlRy3ce4rkvNvPVxmxaRkXw8MiejEvuSMMIBbtIbfMs4M1sGjAKyHLO9fGqHQkMKdsO8NwXW1i0JYeWURE8eFUPxg/tpDVQRXzk5W/fq8DzwAwP2xAfOedYvPUAz3y+mZTtB4lp3ICHR/bkp8nxWk1JJAB49lvonPvGzDp59fPFP6dGxTz7+WZSdx6iVdMG/PZHvRg7JJ7I+joVIxIofD/MMrPJwGSA+Ph4n6uRqpSUOBamZ/LCV1tJyzhM22aRPDa6NzcmdlCwiwQg3wPeOTcFmAKQmJjofC5HKlBQVMw7K/fw4jfb2JZ9jA4tG/L49Rfy44HttZKSSADzPeAlcOWeOMnslF1M+8d2Mo8W0LttU54bO4Cr+7TWOHaRIKCAl3+SlXuCV/6xg5lLdpJ7ooiLu0bzxI39GNY1RlMKiAQRL4dJzgEuAWLMbDfwW+fcVK/ak/O3NTuPl7/dzlsrdnOyuISRfdpwxw8T6Nu+ud+licg58HIUzVivfrbUHOcci7bkMG3Rdr7cmE1EeD1+PLA9k4cn0Dkmyu/yROQ86BRNHXXiZOmF02n/2M6mzDxiGjfgl5d35+akeGKbNPC7PBGpAQr4Oibr6AleW7KTWSm7OHiskF5tmvLEjf34Ub82midGJMQo4OuIVRmHefW7HSxYvZeiEscVPVtx+7DOJHVuqQunIiFKAR/CjhcW8/6qvcxM2cnq3UeIighjXHJHbr2oEx2jdX5dJNQp4EPQtuw8ZqXs4o3UDI6eKKJ7q8Y8Nro31w5oR5PI+n6XJyK1RAEfIoqKS/gsPZOZS3axaEsO9cOMq/q0YVxSPEN0GkakTlLAB7ndh/J5I3U385ZlsP/oCdo2i+SBK7tz0+AOxDWJ9Ls8EfGRAj4IFRQV8+m6TF5PzWDRlhwAhnWN4fejezPigjhNIyAigAI+qKTvO8q8ZRm8k7aHw/knade8IfeO6MaNie1p36KR3+WJSIBRwAe4oydO8l7aXl5PzWD17iNEhNXjit6t+EliBy7uGkNYPZ1bF5GKKeADUGFRCd9symZ+2h4+W59JQVEJF7RuwiOjenHdgHa0iIrwu0QRCQIK+ADhnGNlxmHeWbmH91ft5VD+SVpGRTBmcAeuH9ievu2baSSMiJwVBbzPtucc452Ve3gnbQ87D+TTILweV/RqxXUD2jG8eyz1dcFURM6RAt4Hew8f58M1+1iweh9pGYcxg6EJ0dxzaVeu6tNaNyOJSI1QwNeSfUeO8+Ga/Xywei8rdh0GoFebpvzn1RdwTf+2tGnW0N8CRSTkKOA9tP/ICT5cs48P1uxj+c5DQGmo/+pfejDywjaab11EPKWAr2E7co6xcH0mn6zbT2pZqPds05QHruzOyAvbkBDb2OcKRaSuUMCfp5ISR9ruwyxcn8ln6zPZnJUHlIb6v1/RnZF929BFoS4iPlDAn4MTJ4v5bmtOaainZ5GdW0BYPSOpc0tuTorn8p6t6NBSd5aKiL8U8NWUcTCfrzdl89XGbL7bmkN+YTFREWFc0iOOK3q14tIecTRrpNEvIhI4FPCVOHGymJTtB/l6YzZfbcpiW/YxANq3aMj1A9txec9WDO0SrWXuRCRgKeDLOOfYmp3Ht5tz+GpjNku2HaCgqISI8HokJ0QzLqkjP+wRS0JMlO4oFZGgUGcD3jnHroP5LN56gO+2HmDxtgNk5xYAkBATxdgh8VzSI5akztE0jNBRuogEnzoV8PuOHOe7LaVhvnjrAfYcPg5AbJMGDE2I5qIu0VzUJYb4aF0gFZHg52nAm9lVwDNAGPCyc+6PXrZ3upISx+asPFJ3HmT5jkOk7jzEroP5ALRoVJ/khGju/GECQ7tE0yW2sU67iEjI8SzgzSwM+CtwBbAbWGZm7znn1nvR3vHCYtIyDrN850FSdx5ixc5DHD1RBEBM4wgGdWzB+KEduahLDBe0bkI9zaMuIiHOyyP4IcAW59w2ADObC4wGajTgC4qKuenFJazbc4SiEgdAt7jG/GvfNgzq2JLEji3oGN1IR+giUud4GfDtgIzTvt8NJJXfycwmA5MB4uPjz7qRBuFhdI5uxMVdokns1IKB8S1o3kgLYoiIeBnwFR0yu396wrkpwBSAxMTEf9peHU+PGXAuLxMRCWleriaxG+hw2vftgb0eticiIqfxMuCXAd3MrLOZRQBjgPc8bE9ERE7j2Ska51yRmd0DfELpMMlpzrl1XrUnIiLf5+k4eOfch8CHXrYhIiIV04rOIiIhSgEvIhKiFPAiIiFKAS8iEqLMuXO6t8gTZpYN7DzHl8cAOTVYTk1RXWcvUGtTXWdHdZ29c6mto3MutqINARXw58PMUp1ziX7XUZ7qOnuBWpvqOjuq6+zVdG06RSMiEqIU8CIiISqUAn6K3wVUQnWdvUCtTXWdHdV19mq0tpA5By8iIt8XSkfwIiJyGgW8iEiICqqAN7OrzGyjmW0xs4cq2G5m9mzZ9tVmNrCW6upgZl+aWbqZrTOz+yrY5xIzO2JmaWVfj9RSbTvMbE1Zm6kVbK/1PjOzHqf1Q5qZHTWz+8vtU2v9ZWbTzCzLzNae9lxLM1toZpvL/m1RyWur/Ex6UNdfzGxD2Xs138yaV/LaKt93D+p61Mz2nPZ+jazktbXdX/NOq2mHmaVV8lov+6vCfKiVz5hzLii+KJ1yeCuQAEQAq4Be5fYZCXxE6WpSyUBKLdXWBhhY9rgJsKmC2i4BFvjQbzuAmCq2+9Jn5d7X/ZTerOFLfwHDgYHA2tOe+zPwUNnjh4A/VVJ7lZ9JD+q6Eggve/yniuqqzvvuQV2PAg9U472u1f4qt/1J4BEf+qvCfKiNz1gwHcH/7yLezrlC4NQi3qcbDcxwpZYAzc2sjdeFOef2OedWlD3OBdIpXZM2GPjSZ6e5DNjqnDvXO5jPm3PuG+BguadHA9PLHk8Hrq3gpdX5TNZoXc65T51zRWXfLqF0pbRaVUl/VUet99cpZmbATcCcmmqvuqrIB88/Y8EU8BUt4l0+RKuzj6fMrBMwAEipYPNQM1tlZh+ZWe9aKskBn5rZcitd4Lw8v/tsDJX/0vnRX6e0cs7tg9JfUCCugn387rvbKf3rqyJnet+9cE/ZqaNplZxu8LO/fgBkOuc2V7K9VvqrXD54/hkLpoCvziLe1Vro2ytm1hh4C7jfOXe03OYVlJ6G6Ac8B7xTS2Vd7JwbCFwN3G1mw8tt963PrHQpx2uANyrY7Fd/nQ0/++5hoAiYVckuZ3rfa9oLQBegP7CP0tMh5fn5+zmWqo/ePe+vM+RDpS+r4Llq91kwBXx1FvH2baFvM6tP6Zs3yzn3dvntzrmjzrm8sscfAvXNLMbrupxze8v+zQLmU/on3+n8XBz9amCFcy6z/Aa/+us0madOVZX9m1XBPr70nZn9DBgF/NSVnagtrxrve41yzmU654qdcyXAS5W051d/hQPXA/Mq28fr/qokHzz/jAVTwFdnEe/3gPFlI0OSgSOn/gTyUtn5valAunPuqUr2aV22H2Y2hNK+P+BxXVFm1uTUY0ov0K0tt5svfVam0qMqP/qrnPeAn5U9/hnwbgX71PrC8mZ2FfAfwDXOufxK9qnO+17TdZ1+3ea6Stqr9f4qczmwwTm3u6KNXvdXFfng/WfMi6vGXn1ROuJjE6VXlR8ue+5O4M6yxwb8tWz7GiCxluoaRumfTauBtLKvkeVquwdYR+lV8CXARbVQV0JZe6vK2g6kPmtEaWA3O+05X/qL0v/J7ANOUnrENAGIBj4HNpf927Js37bAh1V9Jj2uawul52RPfc7+Xr6uyt53j+t6rezzs5rSAGoTCP1V9vyrpz5Xp+1bm/1VWT54/hnTVAUiIiEqmE7RiIjIWVDAi4iEKAW8iEiIUsCLiIQoBbyISIhSwIuIhCgFvIhIiFLAi1TCzAaXTZ4VWXa34zoz6+N3XSLVpRudRKpgZn8AIoGGwG7n3OM+lyRSbQp4kSqUzf+xDDhB6XQJxT6XJFJtOkUjUrWWQGNKV+KJ9LkWkbOiI3iRKpjZe5SuotOZ0gm07vG5JJFqC/e7AJFAZWbjgSLn3GwzCwO+M7MRzrkv/K5NpDp0BC8iEqJ0Dl5EJEQp4EVEQpQCXkQkRCngRURClAJeRCREKeBFREKUAl5EJET9fy3Z/7BKPv6hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "print(numerical_diff(function_1, 5))\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c488cc",
   "metadata": {},
   "source": [
    "편미분\n",
    "- 변수가 여럿인 함수에 대한 미분을 편미분이라고 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d116dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n",
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0] **2 + x[1] ** 2\n",
    "\n",
    "def function_tmp1(x0):\n",
    "    return x0 * x0 + 4.0 **2.0\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0 ** 2.0 + x1 * x1\n",
    "\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))\n",
    "print(numerical_diff(function_tmp2, 4.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da1423",
   "metadata": {},
   "source": [
    "기울기 \n",
    "- 모든 변수의 편미분을 벡터로 정리한 것을 기울기라고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1742adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x): \n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        #f(x+h) 계산 \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        #f(x-h) 계산 \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1f3add11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient(function_2, np.array([3.0,4.0])))\n",
    "print(numerical_gradient(function_2, np.array([0.0,2.0])))\n",
    "print(numerical_gradient(function_2, np.array([3.0,0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43250258",
   "metadata": {},
   "source": [
    "경사법 \n",
    "- 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지를 짐작할 수 없기에 기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사법입니다.\n",
    "- 기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나 그 방향으로 가야 함수의 값을 줄일 수 있습니다. 그래서 최솟값이 되는 장소를 찾는 문제에서는 기울기 정보를 단서로 나아갈 방향을 정해야 합니다. \n",
    "- 경사법은 현 위치에서 기울어진 방향으로 일정 기리만큼 이동합니다. 그런 다음 이동한 곳에서도 또 기울기를 구하고 그 기울어진 방향으로 나아가기를 반복합니다. \n",
    "- 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법입니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "835bb932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a5ffbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n",
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr = 0.1, step_num=100))\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr = 10, step_num=100))\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr = 1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df672d7a",
   "metadata": {},
   "source": [
    "신경망에서도 기울기를 구합니다. 가중치가 W, 손실 함수 L인 신경망을 생각했을 때, 손실 함수를 가중치의 각 각의 원소에 대하여 편미분을 구합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05a0643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb9237d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2167819  -0.29208002  2.08808344]\n",
      " [ 0.09768684 -0.26888471 -1.23528172]]\n",
      "[-0.04215099 -0.41724425  0.14109651]\n",
      "2\n",
      "0.8774331058689736\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "print(net.W)\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p))\n",
    "t = np.array([0,0,1])\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3eb674e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20773196  0.14275863 -0.35049059]\n",
      " [ 0.31159793  0.21413795 -0.52573588]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6e1c3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습 알고리즘 구현하기 \n",
    "#1. 미니배치 \n",
    "#2. 기울기 산출 \n",
    "#3. 매개변수 갱시 \n",
    "#4. 반복 (1~3)\n",
    "## 경사 하강법을 미니배치로 무작위 선하기 때문에 stochastic gradient descent라고도 합니다. \n",
    "## 2층 신경망 클래스 구현 \n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d554085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09925, 0.1008\n",
      "train acc, test acc | 0.7847166666666666, 0.7902\n",
      "train acc, test acc | 0.8761166666666667, 0.8798\n",
      "train acc, test acc | 0.89825, 0.9021\n",
      "train acc, test acc | 0.9084333333333333, 0.9127\n",
      "train acc, test acc | 0.9151833333333333, 0.9182\n",
      "train acc, test acc | 0.9203333333333333, 0.924\n",
      "train acc, test acc | 0.92445, 0.9269\n",
      "train acc, test acc | 0.9286333333333333, 0.93\n",
      "train acc, test acc | 0.9312166666666667, 0.932\n",
      "train acc, test acc | 0.9345333333333333, 0.9374\n",
      "train acc, test acc | 0.9372833333333334, 0.9391\n",
      "train acc, test acc | 0.9401, 0.9411\n",
      "train acc, test acc | 0.9418333333333333, 0.942\n",
      "train acc, test acc | 0.94405, 0.9444\n",
      "train acc, test acc | 0.94595, 0.9461\n",
      "train acc, test acc | 0.9474, 0.9466\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67845b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
